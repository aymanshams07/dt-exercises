{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of torchvision_finetuning_instance_segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7593f080af834360a38c67566577eba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_048da92bb59c4fd5ab8bb23c5bb8d866",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2d234a4985464432a240b165ee8b2af9",
              "IPY_MODEL_d09e0a68d9bf4fbcb851d2db66bc44b8"
            ]
          }
        },
        "048da92bb59c4fd5ab8bb23c5bb8d866": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d234a4985464432a240b165ee8b2af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f6902cec30364bc2b51e6b9293824519",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 167502836,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 167502836,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f4264f96a2c94eb88567b2992e619992"
          }
        },
        "d09e0a68d9bf4fbcb851d2db66bc44b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cb2f3228bbdb4d15bbeb295086151678",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 160M/160M [01:30&lt;00:00, 1.85MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_295e0e3cb47346e28633613dc8ada531"
          }
        },
        "f6902cec30364bc2b51e6b9293824519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f4264f96a2c94eb88567b2992e619992": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb2f3228bbdb4d15bbeb295086151678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "295e0e3cb47346e28633613dc8ada531": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfPPQ6ztJhv4"
      },
      "source": [
        "# TorchVision 0.3 Object Detection finetuning tutorial\n",
        "\n",
        "For this tutorial, we will be finetuning a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870) model in the [*Penn-Fudan Database for Pedestrian Detection and Segmentation*](https://www.cis.upenn.edu/~jshi/ped_html/). It contains 170 images with 345 instances of pedestrians, and we will use it to illustrate how to use the new features in torchvision in order to train an instance segmentation model on a custom dataset.\n",
        "\n",
        "First, we need to install `pycocotools`. This library will be used for computing the evaluation metrics following the COCO metric for intersection over union."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBIoe_tHTQgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17690d91-8ab3-43a3-ce57-6551dc922705"
      },
      "source": [
        "%%shell\n",
        "\n",
        "pip install cython\n",
        "# Install pycocotools, the version by default in Colab\n",
        "# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n",
        "pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (0.29.21)\n",
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-etln6ded\n",
            "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-etln6ded\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (51.0.0)\n",
            "Requirement already satisfied, skipping upgrade: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.21)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.19.4)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools==2.0) (1.15.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=265564 sha256=fe78fcc3c5e7c100329912640de8d9a1af94d4f65148958ab13f3d41e0fae0bf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xrmgzk60/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3a\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "  Found existing installation: pycocotools 2.0.2\n",
            "    Uninstalling pycocotools-2.0.2:\n",
            "      Successfully uninstalled pycocotools-2.0.2\n",
            "Successfully installed pycocotools-2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sd4jlGp2eLm"
      },
      "source": [
        "## Defining the Dataset\n",
        "\n",
        "The [torchvision reference scripts for training object detection, instance segmentation and person keypoint detection](https://github.com/pytorch/vision/tree/v0.3.0/references/detection) allows for easily supporting adding new custom datasets.\n",
        "The dataset should inherit from the standard `torch.utils.data.Dataset` class, and implement `__len__` and `__getitem__`.\n",
        "\n",
        "The only specificity that we require is that the dataset `__getitem__` should return:\n",
        "\n",
        "* image: a PIL Image of size (H, W)\n",
        "* target: a dict containing the following fields\n",
        "    * `boxes` (`FloatTensor[N, 4]`): the coordinates of the `N` bounding boxes in `[x0, y0, x1, y1]` format, ranging from `0` to `W` and `0` to `H`\n",
        "    * `labels` (`Int64Tensor[N]`): the label for each bounding box\n",
        "    * `image_id` (`Int64Tensor[1]`): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n",
        "    * `area` (`Tensor[N]`): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n",
        "    * `iscrowd` (`UInt8Tensor[N]`): instances with `iscrowd=True` will be ignored during evaluation.\n",
        "    * (optionally) `masks` (`UInt8Tensor[N, H, W]`): The segmentation masks for each one of the objects\n",
        "    * (optionally) `keypoints` (`FloatTensor[N, K, 3]`): For each one of the `N` objects, it contains the `K` keypoints in `[x, y, visibility]` format, defining the object. `visibility=0` means that the keypoint is not visible. Note that for data augmentation, the notion of flipping a keypoint is dependent on the data representation, and you should probably adapt `references/detection/transforms.py` for your new keypoint representation\n",
        "\n",
        "If your model returns the above methods, they will make it work for both training and evaluation, and will use the evaluation scripts from pycocotools.\n",
        "\n",
        "Additionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a `get_height_and_width` method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via `__getitem__` , which loads the image in memory and is slower than if a custom method is provided.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX0rqK-A3Nbl"
      },
      "source": [
        "### Writing a custom dataset for Duckie Town\n",
        "\n",
        "Let's write a dataset for the Penn-Fudan dataset.\n",
        "\n",
        "First, let's download and extract the data, present in a zip file at https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDjuVFgexFfh"
      },
      "source": [
        "from PIL import Image\n",
        "#Image.open('PennFudanPed/PNGImages/FudanPed00001.png')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Ee5NV54Dmj"
      },
      "source": [
        "So each image has a corresponding segmentation mask, where each color correspond to a different instance. Let's write a `torch.utils.data.Dataset` class for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqeQLrDvhiFn",
        "outputId": "6aff62a2-b738-4024-ee0a-315a3a6a0143"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFAVOm58iDf8",
        "outputId": "7b24f2bd-b95e-489e-9042-807313aed1f5"
      },
      "source": [
        "!ls \"/content/drive/MyDrive/dataset\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.npz\t 14.npz   19.npz   249.npz  299.npz  348.npz  398.npz  447.npz\t53.npz\n",
            "100.npz  150.npz  1.npz    24.npz   29.npz   349.npz  399.npz  448.npz\t54.npz\n",
            "101.npz  151.npz  200.npz  250.npz  2.npz    34.npz   39.npz   449.npz\t55.npz\n",
            "102.npz  152.npz  201.npz  251.npz  300.npz  350.npz  3.npz    44.npz\t56.npz\n",
            "103.npz  153.npz  202.npz  252.npz  301.npz  351.npz  400.npz  450.npz\t57.npz\n",
            "104.npz  154.npz  203.npz  253.npz  302.npz  352.npz  401.npz  451.npz\t58.npz\n",
            "105.npz  155.npz  204.npz  254.npz  303.npz  353.npz  402.npz  452.npz\t59.npz\n",
            "106.npz  156.npz  205.npz  255.npz  304.npz  354.npz  403.npz  453.npz\t5.npz\n",
            "107.npz  157.npz  206.npz  256.npz  305.npz  355.npz  404.npz  454.npz\t60.npz\n",
            "108.npz  158.npz  207.npz  257.npz  306.npz  356.npz  405.npz  455.npz\t61.npz\n",
            "109.npz  159.npz  208.npz  258.npz  307.npz  357.npz  406.npz  456.npz\t62.npz\n",
            "10.npz\t 15.npz   209.npz  259.npz  308.npz  358.npz  407.npz  457.npz\t63.npz\n",
            "110.npz  160.npz  20.npz   25.npz   309.npz  359.npz  408.npz  458.npz\t64.npz\n",
            "111.npz  161.npz  210.npz  260.npz  30.npz   35.npz   409.npz  459.npz\t65.npz\n",
            "112.npz  162.npz  211.npz  261.npz  310.npz  360.npz  40.npz   45.npz\t66.npz\n",
            "113.npz  163.npz  212.npz  262.npz  311.npz  361.npz  410.npz  460.npz\t67.npz\n",
            "114.npz  164.npz  213.npz  263.npz  312.npz  362.npz  411.npz  461.npz\t68.npz\n",
            "115.npz  165.npz  214.npz  264.npz  313.npz  363.npz  412.npz  462.npz\t69.npz\n",
            "116.npz  166.npz  215.npz  265.npz  314.npz  364.npz  413.npz  463.npz\t6.npz\n",
            "117.npz  167.npz  216.npz  266.npz  315.npz  365.npz  414.npz  464.npz\t70.npz\n",
            "118.npz  168.npz  217.npz  267.npz  316.npz  366.npz  415.npz  465.npz\t71.npz\n",
            "119.npz  169.npz  218.npz  268.npz  317.npz  367.npz  416.npz  466.npz\t72.npz\n",
            "11.npz\t 16.npz   219.npz  269.npz  318.npz  368.npz  417.npz  467.npz\t73.npz\n",
            "120.npz  170.npz  21.npz   26.npz   319.npz  369.npz  418.npz  468.npz\t74.npz\n",
            "121.npz  171.npz  220.npz  270.npz  31.npz   36.npz   419.npz  469.npz\t75.npz\n",
            "122.npz  172.npz  221.npz  271.npz  320.npz  370.npz  41.npz   46.npz\t76.npz\n",
            "123.npz  173.npz  222.npz  272.npz  321.npz  371.npz  420.npz  470.npz\t77.npz\n",
            "124.npz  174.npz  223.npz  273.npz  322.npz  372.npz  421.npz  471.npz\t78.npz\n",
            "125.npz  175.npz  224.npz  274.npz  323.npz  373.npz  422.npz  472.npz\t79.npz\n",
            "126.npz  176.npz  225.npz  275.npz  324.npz  374.npz  423.npz  473.npz\t7.npz\n",
            "127.npz  177.npz  226.npz  276.npz  325.npz  375.npz  424.npz  474.npz\t80.npz\n",
            "128.npz  178.npz  227.npz  277.npz  326.npz  376.npz  425.npz  475.npz\t81.npz\n",
            "129.npz  179.npz  228.npz  278.npz  327.npz  377.npz  426.npz  476.npz\t82.npz\n",
            "12.npz\t 17.npz   229.npz  279.npz  328.npz  378.npz  427.npz  477.npz\t83.npz\n",
            "130.npz  180.npz  22.npz   27.npz   329.npz  379.npz  428.npz  478.npz\t84.npz\n",
            "131.npz  181.npz  230.npz  280.npz  32.npz   37.npz   429.npz  479.npz\t85.npz\n",
            "132.npz  182.npz  231.npz  281.npz  330.npz  380.npz  42.npz   47.npz\t86.npz\n",
            "133.npz  183.npz  232.npz  282.npz  331.npz  381.npz  430.npz  480.npz\t87.npz\n",
            "134.npz  184.npz  233.npz  283.npz  332.npz  382.npz  431.npz  481.npz\t88.npz\n",
            "135.npz  185.npz  234.npz  284.npz  333.npz  383.npz  432.npz  482.npz\t89.npz\n",
            "136.npz  186.npz  235.npz  285.npz  334.npz  384.npz  433.npz  483.npz\t8.npz\n",
            "137.npz  187.npz  236.npz  286.npz  335.npz  385.npz  434.npz  484.npz\t90.npz\n",
            "138.npz  188.npz  237.npz  287.npz  336.npz  386.npz  435.npz  485.npz\t91.npz\n",
            "139.npz  189.npz  238.npz  288.npz  337.npz  387.npz  436.npz  486.npz\t92.npz\n",
            "13.npz\t 18.npz   239.npz  289.npz  338.npz  388.npz  437.npz  487.npz\t93.npz\n",
            "140.npz  190.npz  23.npz   28.npz   339.npz  389.npz  438.npz  488.npz\t94.npz\n",
            "141.npz  191.npz  240.npz  290.npz  33.npz   38.npz   439.npz  489.npz\t95.npz\n",
            "142.npz  192.npz  241.npz  291.npz  340.npz  390.npz  43.npz   48.npz\t96.npz\n",
            "143.npz  193.npz  242.npz  292.npz  341.npz  391.npz  440.npz  490.npz\t97.npz\n",
            "144.npz  194.npz  243.npz  293.npz  342.npz  392.npz  441.npz  491.npz\t98.npz\n",
            "145.npz  195.npz  244.npz  294.npz  343.npz  393.npz  442.npz  49.npz\t99.npz\n",
            "146.npz  196.npz  245.npz  295.npz  344.npz  394.npz  443.npz  4.npz\t9.npz\n",
            "147.npz  197.npz  246.npz  296.npz  345.npz  395.npz  444.npz  50.npz\n",
            "148.npz  198.npz  247.npz  297.npz  346.npz  396.npz  445.npz  51.npz\n",
            "149.npz  199.npz  248.npz  298.npz  347.npz  397.npz  446.npz  52.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tejevd6InImh"
      },
      "source": [
        "import numpy as np\n",
        "dataset_1 = np.load('/content/drive/MyDrive/dataset/0.npz')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLAWlU-GoLNh",
        "outputId": "5e7ebca3-308f-4340-c3d0-4f028e1cc844"
      },
      "source": [
        "dataset_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<numpy.lib.npyio.NpzFile at 0x7f7c5235de10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FybCa1AHKmqK",
        "outputId": "7177a269-7c9c-48ed-9997-40ae3ea17d19"
      },
      "source": [
        "dataset_1.files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['arr_0', 'arr_1', 'arr_2']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaemZWuiMCaZ",
        "outputId": "e32be848-d19d-4d98-d733-f3ae644ce6e3"
      },
      "source": [
        "dataset_1['arr_1']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[153,  35, 223,  87]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3FTe-QUc1wo"
      },
      "source": [
        "npz_file_path = '/content/drive/MyDrive/dataset/0.npz'\n",
        "npz_file_dict = np.load(npz_file_path)\n",
        "num_records = len(npz_file_dict['arr_0'])\n",
        "\n",
        "img = npz_file_dict[f\"arr_{0}\"]\n",
        "boxes = npz_file_dict[f\"arr_{1}\"]\n",
        "classes = npz_file_dict[f\"arr_{2}\"]\n",
        "\n",
        "\n",
        "def get_items(num_records):\n",
        "   for i in range(num_records):\n",
        "      return img[i], boxes[i], classes[i]\n",
        "# for i in glob.glob('*.npz'):\n",
        "#     numpy_vars[np_name] = np.load(np_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPhJOod4ecCk",
        "outputId": "44c30856-b51c-463f-87cd-ad10476c3aab"
      },
      "source": [
        "  img.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUM2aNv6fD8S",
        "outputId": "0ed88cc8-8524-4e00-eb7f-4b0bbe72b122"
      },
      "source": [
        "num_records"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "224"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTgWtixZTs3X"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class DuckieDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root,  transforms=None):\n",
        "        # initialization\n",
        "        self.root = root\n",
        "        #self.list_IDs = list_IDs\n",
        "        self.transforms = transforms\n",
        "        self.path = '/content/drive/MyDrive/dataset/'\n",
        "        #self.imgs = list(sorted(os.listdir(path)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.path)\n",
        "        \n",
        " \n",
        "    def __getitem__(self, idx):\n",
        "        # load images ad masks\n",
        "        #ID = self.list_IDs[idx]\n",
        "        \n",
        "        # if torch.is_tensor(idx):\n",
        "        #     idx = idx.tolist()\n",
        "        \n",
        "        # load data (not sure if we do np.load, or torch.load)\n",
        "        npz_file_path = '/content/drive/MyDrive/dataset/'\n",
        "        npz_file_dict = np.load(npz_file_path + str(idx) + '.npz')\n",
        "        img = npz_file_dict[f\"arr_{0}\"]\n",
        "        boxes = npz_file_dict[f\"arr_{1}\"]\n",
        "        classes = npz_file_dict[f\"arr_{2}\"]\n",
        "        #img = Image.open(img).convert('RGB')\n",
        "        \n",
        "        #img = img.transpose(2,0,1)\n",
        "        img = img.reshape(3,224, 224)\n",
        "        img = torch.as_tensor(img, dtype=torch.float32)\n",
        "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0]) \n",
        "        boxes = boxes[keep] \n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        classes = classes\n",
        "        labels = torch.as_tensor((classes), dtype=torch.int64)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6f3ZOTJ4Km9"
      },
      "source": [
        "That's all for the dataset. Let's see how the outputs are structured for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEARO4B_ye0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc4e41ef-0033-49e4-a691-8caa45d005ef"
      },
      "source": [
        "dataset = DuckieDataset('/content/drive/MyDrive/dataset/')\n",
        "\n",
        "dataset[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[255.,   0., 255.,  ..., 255., 255.,   0.],\n",
              "          [255., 255.,   0.,  ...,   0., 255., 255.],\n",
              "          [  0., 255., 255.,  ..., 255.,   0., 255.],\n",
              "          ...,\n",
              "          [  0., 255., 255.,  ..., 116., 114., 117.],\n",
              "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,  ...,   0., 164., 219.]],\n",
              " \n",
              "         [[  0., 219., 219.,  ..., 116., 114., 117.],\n",
              "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
              "          ...,\n",
              "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
              " \n",
              "         [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
              "          ...,\n",
              "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
              "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]]),\n",
              " {'boxes': tensor([[153.,  35., 223.,  87.]]),\n",
              "  'image_id': tensor([0]),\n",
              "  'labels': tensor([3])})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWOhcsir9Ahx"
      },
      "source": [
        "So we can see that by default, the dataset returns a `PIL.Image` and a dictionary\n",
        "containing several fields, including `boxes`, `labels` and `masks`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoAEkUgn4uEq"
      },
      "source": [
        "## Defining your model\n",
        "\n",
        "In this tutorial, we will be using [Mask R-CNN](https://arxiv.org/abs/1703.06870), which is based on top of [Faster R-CNN](https://arxiv.org/abs/1506.01497). Faster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image.\n",
        "\n",
        "![Faster R-CNN](https://raw.githubusercontent.com/pytorch/vision/temp-tutorial/tutorials/tv_image03.png)\n",
        "\n",
        "Mask R-CNN adds an extra branch into Faster R-CNN, which also predicts segmentation masks for each instance.\n",
        "\n",
        "![Mask R-CNN](https://raw.githubusercontent.com/pytorch/vision/temp-tutorial/tutorials/tv_image04.png)\n",
        "\n",
        "There are two common situations where one might want to modify one of the available models in torchvision modelzoo.\n",
        "The first is when we want to start from a pre-trained model, and just finetune the last layer. The other is when we want to replace the backbone of the model with a different one (for faster predictions, for example).\n",
        "\n",
        "Let's go see how we would do one or another in the following sections.\n",
        "\n",
        "\n",
        "### 1 - Finetuning from a pretrained model\n",
        "\n",
        "Let's suppose that you want to start from a model pre-trained on COCO and want to finetune it for your particular classes. Here is a possible way of doing it:\n",
        "```\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# load a model pre-trained pre-trained on COCO\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# replace the classifier with a new one, that has\n",
        "# num_classes which is user-defined\n",
        "num_classes = 2  # 1 class (person) + background\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
        "```\n",
        "\n",
        "### 2 - Modifying the model to add a different backbone\n",
        "\n",
        "Another common situation arises when the user wants to replace the backbone of a detection\n",
        "model with a different one. For example, the current default backbone (ResNet-50) might be too big for some applications, and smaller models might be necessary.\n",
        "\n",
        "Here is how we would go into leveraging the functions provided by torchvision to modify a backbone.\n",
        "\n",
        "```\n",
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "# load a pre-trained model for classification and return\n",
        "# only the features\n",
        "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "# FasterRCNN needs to know the number of\n",
        "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
        "# so we need to add it here\n",
        "backbone.out_channels = 1280\n",
        "\n",
        "# let's make the RPN generate 5 x 3 anchors per spatial\n",
        "# location, with 5 different sizes and 3 different aspect\n",
        "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
        "# map could potentially have different sizes and\n",
        "# aspect ratios \n",
        "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "# let's define what are the feature maps that we will\n",
        "# use to perform the region of interest cropping, as well as\n",
        "# the size of the crop after rescaling.\n",
        "# if your backbone returns a Tensor, featmap_names is expected to\n",
        "# be [0]. More generally, the backbone should return an\n",
        "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
        "# feature maps to use.\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
        "                                                output_size=7,\n",
        "                                                sampling_ratio=2)\n",
        "\n",
        "# put the pieces together inside a FasterRCNN model\n",
        "model = FasterRCNN(backbone,\n",
        "                   num_classes=2,\n",
        "                   rpn_anchor_generator=anchor_generator,\n",
        "                   box_roi_pool=roi_pooler)\n",
        "```\n",
        "\n",
        "### An Instance segmentation model for PennFudan Dataset\n",
        "\n",
        "In our case, we want to fine-tune from a pre-trained model, given that our dataset is very small. So we will be following approach number 1.\n",
        "\n",
        "Here we want to also compute the instance segmentation masks, so we will be using Mask R-CNN:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjNHjVMOyYlH"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "      \n",
        "def get_instance_segmentation_model(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    \n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 5)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WXLwePV5ieP"
      },
      "source": [
        "That's it, this will make model be ready to be trained and evaluated on our custom dataset.\n",
        "\n",
        "## Training and evaluation functions\n",
        "\n",
        "In `references/detection/,` we have a number of helper functions to simplify training and evaluating detection models.\n",
        "Here, we will use `references/detection/engine.py`, `references/detection/utils.py` and `references/detection/transforms.py`.\n",
        "\n",
        "Let's copy those files (and their dependencies) in here so that they are available in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYDb7PBw55b-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ee47c65-a9b2-434d-adba-9400e49a1b00"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 12256 (delta 3), reused 1 (delta 0), pack-reused 12249\u001b[K\n",
            "Receiving objects: 100% (12256/12256), 13.47 MiB | 9.49 MiB/s, done.\n",
            "Resolving deltas: 100% (8549/8549), done.\n",
            "Note: checking out 'v0.3.0'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at be37608 version check against PyTorch's CUDA version\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u9e_pdv54nG"
      },
      "source": [
        "\n",
        "\n",
        "Let's write some helper functions for data augmentation / transformation, which leverages the functions in `refereces/detection` that we have just copied:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l79ivkwKy357"
      },
      "source": [
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        # during training, randomly flip the training images\n",
        "        # and ground-truth for data augmentation\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzCLqiZk-sjf"
      },
      "source": [
        "#### Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YFJGJxk6XEs"
      },
      "source": [
        "### Putting everything together\n",
        "\n",
        "We now have the dataset class, the models and the data transforms. Let's instantiate them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5dGaIezze3y"
      },
      "source": [
        "# use our dataset and defined transformations\n",
        "dataset = DuckieDataset(get_transform(train=True))\n",
        "dataset_test = DuckieDataset(get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "# indices = torch.randperm(len(dataset)).tolist()\n",
        "# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
        "# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "dataset = torch.utils.data.Subset(dataset, range(0, 400))\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, range(300,400))\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5yvZUprj4ZN"
      },
      "source": [
        "Now let's instantiate the model and the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoenkCj18C4h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "7593f080af834360a38c67566577eba3",
            "048da92bb59c4fd5ab8bb23c5bb8d866",
            "2d234a4985464432a240b165ee8b2af9",
            "d09e0a68d9bf4fbcb851d2db66bc44b8",
            "f6902cec30364bc2b51e6b9293824519",
            "f4264f96a2c94eb88567b2992e619992",
            "cb2f3228bbdb4d15bbeb295086151678",
            "295e0e3cb47346e28633613dc8ada531"
          ]
        },
        "outputId": "99e0a672-557a-4553-9aa3-49b9bd8684e9"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# our dataset has two classes only - background and person\n",
        "num_classes = 2\n",
        "\n",
        "# get the model using our helper function\n",
        "model = get_instance_segmentation_model(num_classes)\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.00001,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7593f080af834360a38c67566577eba3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=167502836.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAd56lt4kDxc"
      },
      "source": [
        "And now let's train the model for 10 epochs, evaluating at the end of every epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at-h4OWK0aoc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961bd96d-528b-4427-be87-60f87fe92ed1"
      },
      "source": [
        "# let's train it for 10 epochs\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    #evaluate(model, data_loader_test, device=device)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0]  [  0/200]  eta: 0:04:55  lr: 0.000000  loss: 218.3300 (218.3300)  loss_classifier: 167.8367 (167.8367)  loss_box_reg: 0.8498 (0.8498)  loss_objectness: 49.2202 (49.2202)  loss_rpn_box_reg: 0.4233 (0.4233)  time: 1.4762  data: 1.0827  max mem: 1619\n",
            "Epoch: [0]  [ 10/200]  eta: 0:00:58  lr: 0.000001  loss: 66.2830 (92.1827)  loss_classifier: 60.8177 (76.7920)  loss_box_reg: 0.5372 (0.7152)  loss_objectness: 4.0338 (13.4769)  loss_rpn_box_reg: 0.8898 (1.1986)  time: 0.3091  data: 0.1468  max mem: 1885\n",
            "Epoch: [0]  [ 20/200]  eta: 0:00:50  lr: 0.000001  loss: 11.9013 (51.7427)  loss_classifier: 1.9477 (40.6691)  loss_box_reg: 0.4525 (0.6207)  loss_objectness: 3.7971 (9.6080)  loss_rpn_box_reg: 0.6284 (0.8450)  time: 0.2196  data: 0.0813  max mem: 1885\n",
            "Epoch: [0]  [ 30/200]  eta: 0:00:43  lr: 0.000002  loss: 4.0108 (37.5020)  loss_classifier: 0.9318 (27.8727)  loss_box_reg: 0.4091 (0.5446)  loss_objectness: 2.7985 (8.2972)  loss_rpn_box_reg: 0.4273 (0.7874)  time: 0.2266  data: 0.0882  max mem: 1885\n",
            "Epoch: [0]  [ 40/200]  eta: 0:00:40  lr: 0.000002  loss: 3.9825 (29.9971)  loss_classifier: 0.7764 (21.3072)  loss_box_reg: 0.2947 (0.4988)  loss_objectness: 1.6632 (7.4738)  loss_rpn_box_reg: 0.3933 (0.7173)  time: 0.2232  data: 0.0847  max mem: 1885\n",
            "Epoch: [0]  [ 50/200]  eta: 0:00:36  lr: 0.000003  loss: 3.2242 (25.0608)  loss_classifier: 1.0555 (17.3990)  loss_box_reg: 0.3077 (0.4829)  loss_objectness: 1.6464 (6.5550)  loss_rpn_box_reg: 0.2741 (0.6238)  time: 0.2223  data: 0.0829  max mem: 1885\n",
            "Epoch: [0]  [ 60/200]  eta: 0:00:33  lr: 0.000003  loss: 3.2188 (21.6192)  loss_classifier: 1.3993 (14.7728)  loss_box_reg: 0.4111 (0.4791)  loss_objectness: 1.4295 (5.8252)  loss_rpn_box_reg: 0.1167 (0.5420)  time: 0.2219  data: 0.0817  max mem: 1885\n",
            "Epoch: [0]  [ 70/200]  eta: 0:00:30  lr: 0.000004  loss: 2.6817 (18.9187)  loss_classifier: 0.8313 (12.8019)  loss_box_reg: 0.3526 (0.4658)  loss_objectness: 1.1294 (5.1732)  loss_rpn_box_reg: 0.0845 (0.4778)  time: 0.2266  data: 0.0865  max mem: 1885\n",
            "Epoch: [0]  [ 80/200]  eta: 0:00:28  lr: 0.000004  loss: 1.6001 (16.7258)  loss_classifier: 0.4698 (11.2647)  loss_box_reg: 0.2653 (0.4322)  loss_objectness: 0.7161 (4.5986)  loss_rpn_box_reg: 0.0813 (0.4303)  time: 0.2236  data: 0.0831  max mem: 1885\n",
            "Epoch: [0]  [ 90/200]  eta: 0:00:26  lr: 0.000005  loss: 0.8286 (14.9780)  loss_classifier: 0.2556 (10.0612)  loss_box_reg: 0.1511 (0.4024)  loss_objectness: 0.3571 (4.1252)  loss_rpn_box_reg: 0.0567 (0.3891)  time: 0.2345  data: 0.0938  max mem: 1885\n",
            "Epoch: [0]  [100/200]  eta: 0:00:23  lr: 0.000005  loss: 0.7549 (13.5923)  loss_classifier: 0.2481 (9.1070)  loss_box_reg: 0.1953 (0.3884)  loss_objectness: 0.1236 (3.7401)  loss_rpn_box_reg: 0.0379 (0.3568)  time: 0.2251  data: 0.0845  max mem: 1885\n",
            "Epoch: [0]  [110/200]  eta: 0:00:21  lr: 0.000006  loss: 0.7549 (12.4907)  loss_classifier: 0.3364 (8.3464)  loss_box_reg: 0.2488 (0.3777)  loss_objectness: 0.1726 (3.4374)  loss_rpn_box_reg: 0.0378 (0.3292)  time: 0.2264  data: 0.0856  max mem: 1885\n",
            "Epoch: [0]  [120/200]  eta: 0:00:18  lr: 0.000006  loss: 0.4775 (11.5011)  loss_classifier: 0.2098 (7.6712)  loss_box_reg: 0.1594 (0.3604)  loss_objectness: 0.0873 (3.1658)  loss_rpn_box_reg: 0.0305 (0.3037)  time: 0.2202  data: 0.0787  max mem: 1885\n",
            "Epoch: [0]  [130/200]  eta: 0:00:16  lr: 0.000007  loss: 0.3866 (10.6572)  loss_classifier: 0.1505 (7.0974)  loss_box_reg: 0.1383 (0.3433)  loss_objectness: 0.0661 (2.9340)  loss_rpn_box_reg: 0.0159 (0.2825)  time: 0.2000  data: 0.0588  max mem: 1885\n",
            "Epoch: [0]  [140/200]  eta: 0:00:13  lr: 0.000007  loss: 0.3438 (9.9381)  loss_classifier: 0.1315 (6.6047)  loss_box_reg: 0.1244 (0.3271)  loss_objectness: 0.1005 (2.7415)  loss_rpn_box_reg: 0.0188 (0.2647)  time: 0.2137  data: 0.0725  max mem: 1885\n",
            "Epoch: [0]  [150/200]  eta: 0:00:11  lr: 0.000008  loss: 0.4258 (9.3145)  loss_classifier: 0.1321 (6.1778)  loss_box_reg: 0.1165 (0.3150)  loss_objectness: 0.1553 (2.5732)  loss_rpn_box_reg: 0.0165 (0.2486)  time: 0.2183  data: 0.0768  max mem: 1885\n",
            "Epoch: [0]  [160/200]  eta: 0:00:09  lr: 0.000008  loss: 0.4360 (8.7628)  loss_classifier: 0.1375 (5.8037)  loss_box_reg: 0.1165 (0.3024)  loss_objectness: 0.1553 (2.4203)  loss_rpn_box_reg: 0.0165 (0.2364)  time: 0.2220  data: 0.0810  max mem: 1885\n",
            "Epoch: [0]  [170/200]  eta: 0:00:06  lr: 0.000009  loss: 0.4252 (8.3006)  loss_classifier: 0.1439 (5.4740)  loss_box_reg: 0.0867 (0.2893)  loss_objectness: 0.1811 (2.3122)  loss_rpn_box_reg: 0.0200 (0.2251)  time: 0.2210  data: 0.0801  max mem: 1885\n",
            "Epoch: [0]  [180/200]  eta: 0:00:04  lr: 0.000009  loss: 0.5134 (7.8850)  loss_classifier: 0.1290 (5.1773)  loss_box_reg: 0.0700 (0.2784)  loss_objectness: 0.2342 (2.2123)  loss_rpn_box_reg: 0.0357 (0.2171)  time: 0.2172  data: 0.0764  max mem: 1885\n",
            "Epoch: [0]  [190/200]  eta: 0:00:02  lr: 0.000010  loss: 0.4364 (7.4963)  loss_classifier: 0.1017 (4.9118)  loss_box_reg: 0.0786 (0.2681)  loss_objectness: 0.1405 (2.1055)  loss_rpn_box_reg: 0.0440 (0.2109)  time: 0.2194  data: 0.0785  max mem: 1885\n",
            "Epoch: [0]  [199/200]  eta: 0:00:00  lr: 0.000010  loss: 0.3413 (7.1733)  loss_classifier: 0.1017 (4.6966)  loss_box_reg: 0.0711 (0.2604)  loss_objectness: 0.0810 (2.0141)  loss_rpn_box_reg: 0.0258 (0.2022)  time: 0.2180  data: 0.0776  max mem: 1885\n",
            "Epoch: [0] Total time: 0:00:45 (0.2264 s / it)\n",
            "Epoch: [1]  [  0/200]  eta: 0:01:05  lr: 0.000010  loss: 0.2579 (0.2579)  loss_classifier: 0.0724 (0.0724)  loss_box_reg: 0.0547 (0.0547)  loss_objectness: 0.1190 (0.1190)  loss_rpn_box_reg: 0.0119 (0.0119)  time: 0.3297  data: 0.1781  max mem: 1885\n",
            "Epoch: [1]  [ 10/200]  eta: 0:00:30  lr: 0.000010  loss: 0.4331 (0.4477)  loss_classifier: 0.0724 (0.1119)  loss_box_reg: 0.0652 (0.0851)  loss_objectness: 0.2251 (0.2001)  loss_rpn_box_reg: 0.0340 (0.0506)  time: 0.1631  data: 0.0193  max mem: 1885\n",
            "Epoch: [1]  [ 20/200]  eta: 0:00:27  lr: 0.000010  loss: 0.3235 (0.4106)  loss_classifier: 0.0801 (0.1118)  loss_box_reg: 0.0652 (0.0851)  loss_objectness: 0.1054 (0.1588)  loss_rpn_box_reg: 0.0301 (0.0549)  time: 0.1464  data: 0.0039  max mem: 1885\n",
            "Epoch: [1]  [ 30/200]  eta: 0:00:25  lr: 0.000010  loss: 0.2903 (0.3851)  loss_classifier: 0.0808 (0.1025)  loss_box_reg: 0.0609 (0.0805)  loss_objectness: 0.0445 (0.1479)  loss_rpn_box_reg: 0.0155 (0.0542)  time: 0.1462  data: 0.0043  max mem: 1885\n",
            "Epoch: [1]  [ 40/200]  eta: 0:00:24  lr: 0.000010  loss: 0.2425 (0.3557)  loss_classifier: 0.0726 (0.0970)  loss_box_reg: 0.0628 (0.0791)  loss_objectness: 0.0445 (0.1311)  loss_rpn_box_reg: 0.0148 (0.0485)  time: 0.1454  data: 0.0043  max mem: 1885\n",
            "Epoch: [1]  [ 50/200]  eta: 0:00:22  lr: 0.000010  loss: 0.2286 (0.3312)  loss_classifier: 0.0671 (0.0905)  loss_box_reg: 0.0637 (0.0743)  loss_objectness: 0.0480 (0.1214)  loss_rpn_box_reg: 0.0138 (0.0450)  time: 0.1456  data: 0.0045  max mem: 1885\n",
            "Epoch: [1]  [ 60/200]  eta: 0:00:20  lr: 0.000010  loss: 0.2282 (0.3128)  loss_classifier: 0.0618 (0.0865)  loss_box_reg: 0.0521 (0.0716)  loss_objectness: 0.0473 (0.1106)  loss_rpn_box_reg: 0.0193 (0.0441)  time: 0.1460  data: 0.0048  max mem: 1885\n",
            "Epoch: [1]  [ 70/200]  eta: 0:00:19  lr: 0.000010  loss: 0.2282 (0.3054)  loss_classifier: 0.0758 (0.0907)  loss_box_reg: 0.0674 (0.0733)  loss_objectness: 0.0293 (0.1006)  loss_rpn_box_reg: 0.0117 (0.0407)  time: 0.1455  data: 0.0046  max mem: 1885\n",
            "Epoch: [1]  [ 80/200]  eta: 0:00:17  lr: 0.000010  loss: 0.1981 (0.2898)  loss_classifier: 0.0731 (0.0882)  loss_box_reg: 0.0604 (0.0711)  loss_objectness: 0.0293 (0.0933)  loss_rpn_box_reg: 0.0095 (0.0371)  time: 0.1452  data: 0.0044  max mem: 1885\n",
            "Epoch: [1]  [ 90/200]  eta: 0:00:16  lr: 0.000010  loss: 0.1834 (0.2810)  loss_classifier: 0.0689 (0.0878)  loss_box_reg: 0.0524 (0.0697)  loss_objectness: 0.0317 (0.0875)  loss_rpn_box_reg: 0.0103 (0.0359)  time: 0.1455  data: 0.0046  max mem: 1885\n",
            "Epoch: [1]  [100/200]  eta: 0:00:14  lr: 0.000010  loss: 0.2162 (0.2763)  loss_classifier: 0.0752 (0.0866)  loss_box_reg: 0.0590 (0.0688)  loss_objectness: 0.0458 (0.0847)  loss_rpn_box_reg: 0.0199 (0.0361)  time: 0.1454  data: 0.0046  max mem: 1885\n",
            "Epoch: [1]  [110/200]  eta: 0:00:13  lr: 0.000010  loss: 0.2219 (0.2724)  loss_classifier: 0.0769 (0.0876)  loss_box_reg: 0.0671 (0.0695)  loss_objectness: 0.0400 (0.0806)  loss_rpn_box_reg: 0.0183 (0.0346)  time: 0.1455  data: 0.0048  max mem: 1885\n",
            "Epoch: [1]  [120/200]  eta: 0:00:11  lr: 0.000010  loss: 0.2101 (0.2666)  loss_classifier: 0.0814 (0.0872)  loss_box_reg: 0.0692 (0.0693)  loss_objectness: 0.0188 (0.0769)  loss_rpn_box_reg: 0.0122 (0.0332)  time: 0.1460  data: 0.0048  max mem: 1885\n",
            "Epoch: [1]  [130/200]  eta: 0:00:10  lr: 0.000010  loss: 0.2127 (0.2630)  loss_classifier: 0.0774 (0.0859)  loss_box_reg: 0.0553 (0.0680)  loss_objectness: 0.0303 (0.0757)  loss_rpn_box_reg: 0.0179 (0.0334)  time: 0.1446  data: 0.0043  max mem: 1885\n",
            "Epoch: [1]  [140/200]  eta: 0:00:08  lr: 0.000010  loss: 0.2064 (0.2583)  loss_classifier: 0.0659 (0.0854)  loss_box_reg: 0.0422 (0.0661)  loss_objectness: 0.0460 (0.0745)  loss_rpn_box_reg: 0.0135 (0.0323)  time: 0.1445  data: 0.0045  max mem: 1885\n",
            "Epoch: [1]  [150/200]  eta: 0:00:07  lr: 0.000010  loss: 0.1942 (0.2569)  loss_classifier: 0.0845 (0.0865)  loss_box_reg: 0.0469 (0.0656)  loss_objectness: 0.0460 (0.0730)  loss_rpn_box_reg: 0.0114 (0.0318)  time: 0.1453  data: 0.0046  max mem: 1885\n",
            "Epoch: [1]  [160/200]  eta: 0:00:05  lr: 0.000010  loss: 0.2133 (0.2541)  loss_classifier: 0.0845 (0.0870)  loss_box_reg: 0.0542 (0.0647)  loss_objectness: 0.0358 (0.0706)  loss_rpn_box_reg: 0.0095 (0.0318)  time: 0.1457  data: 0.0044  max mem: 1885\n",
            "Epoch: [1]  [170/200]  eta: 0:00:04  lr: 0.000010  loss: 0.1932 (0.2527)  loss_classifier: 0.0749 (0.0861)  loss_box_reg: 0.0522 (0.0641)  loss_objectness: 0.0345 (0.0707)  loss_rpn_box_reg: 0.0098 (0.0317)  time: 0.1459  data: 0.0046  max mem: 1885\n",
            "Epoch: [1]  [180/200]  eta: 0:00:02  lr: 0.000010  loss: 0.2124 (0.2508)  loss_classifier: 0.0748 (0.0865)  loss_box_reg: 0.0522 (0.0637)  loss_objectness: 0.0418 (0.0700)  loss_rpn_box_reg: 0.0078 (0.0306)  time: 0.1453  data: 0.0046  max mem: 1885\n",
            "Epoch: [1]  [190/200]  eta: 0:00:01  lr: 0.000010  loss: 0.1946 (0.2486)  loss_classifier: 0.0748 (0.0867)  loss_box_reg: 0.0539 (0.0633)  loss_objectness: 0.0371 (0.0687)  loss_rpn_box_reg: 0.0075 (0.0299)  time: 0.1459  data: 0.0046  max mem: 1885\n",
            "Epoch: [1]  [199/200]  eta: 0:00:00  lr: 0.000010  loss: 0.1946 (0.2472)  loss_classifier: 0.0716 (0.0866)  loss_box_reg: 0.0539 (0.0630)  loss_objectness: 0.0304 (0.0675)  loss_rpn_box_reg: 0.0088 (0.0300)  time: 0.1463  data: 0.0046  max mem: 1885\n",
            "Epoch: [1] Total time: 0:00:29 (0.1471 s / it)\n",
            "Epoch: [2]  [  0/200]  eta: 0:01:09  lr: 0.000010  loss: 0.1826 (0.1826)  loss_classifier: 0.0583 (0.0583)  loss_box_reg: 0.0665 (0.0665)  loss_objectness: 0.0360 (0.0360)  loss_rpn_box_reg: 0.0217 (0.0217)  time: 0.3478  data: 0.1975  max mem: 1885\n",
            "Epoch: [2]  [ 10/200]  eta: 0:00:30  lr: 0.000010  loss: 0.1826 (0.2176)  loss_classifier: 0.0748 (0.0845)  loss_box_reg: 0.0582 (0.0551)  loss_objectness: 0.0353 (0.0652)  loss_rpn_box_reg: 0.0131 (0.0128)  time: 0.1619  data: 0.0201  max mem: 1885\n",
            "Epoch: [2]  [ 20/200]  eta: 0:00:27  lr: 0.000010  loss: 0.1964 (0.2092)  loss_classifier: 0.0748 (0.0779)  loss_box_reg: 0.0555 (0.0618)  loss_objectness: 0.0298 (0.0537)  loss_rpn_box_reg: 0.0082 (0.0158)  time: 0.1444  data: 0.0036  max mem: 1885\n",
            "Epoch: [2]  [ 30/200]  eta: 0:00:25  lr: 0.000010  loss: 0.1884 (0.1999)  loss_classifier: 0.0633 (0.0730)  loss_box_reg: 0.0501 (0.0565)  loss_objectness: 0.0396 (0.0518)  loss_rpn_box_reg: 0.0082 (0.0185)  time: 0.1454  data: 0.0045  max mem: 1885\n",
            "Epoch: [2]  [ 40/200]  eta: 0:00:23  lr: 0.000010  loss: 0.1819 (0.1940)  loss_classifier: 0.0506 (0.0688)  loss_box_reg: 0.0443 (0.0527)  loss_objectness: 0.0396 (0.0509)  loss_rpn_box_reg: 0.0070 (0.0216)  time: 0.1457  data: 0.0043  max mem: 1885\n",
            "Epoch: [2]  [ 50/200]  eta: 0:00:22  lr: 0.000010  loss: 0.1946 (0.1943)  loss_classifier: 0.0799 (0.0722)  loss_box_reg: 0.0519 (0.0559)  loss_objectness: 0.0297 (0.0472)  loss_rpn_box_reg: 0.0065 (0.0190)  time: 0.1472  data: 0.0045  max mem: 1885\n",
            "Epoch: [2]  [ 60/200]  eta: 0:00:20  lr: 0.000010  loss: 0.1946 (0.1923)  loss_classifier: 0.0752 (0.0707)  loss_box_reg: 0.0491 (0.0539)  loss_objectness: 0.0332 (0.0472)  loss_rpn_box_reg: 0.0082 (0.0206)  time: 0.1470  data: 0.0046  max mem: 1885\n",
            "Epoch: [2]  [ 70/200]  eta: 0:00:19  lr: 0.000010  loss: 0.1569 (0.1906)  loss_classifier: 0.0583 (0.0715)  loss_box_reg: 0.0400 (0.0535)  loss_objectness: 0.0278 (0.0439)  loss_rpn_box_reg: 0.0159 (0.0218)  time: 0.1462  data: 0.0048  max mem: 1885\n",
            "Epoch: [2]  [ 80/200]  eta: 0:00:17  lr: 0.000010  loss: 0.1579 (0.1884)  loss_classifier: 0.0654 (0.0705)  loss_box_reg: 0.0476 (0.0529)  loss_objectness: 0.0262 (0.0434)  loss_rpn_box_reg: 0.0159 (0.0215)  time: 0.1461  data: 0.0047  max mem: 1885\n",
            "Epoch: [2]  [ 90/200]  eta: 0:00:16  lr: 0.000010  loss: 0.1740 (0.1886)  loss_classifier: 0.0629 (0.0695)  loss_box_reg: 0.0512 (0.0536)  loss_objectness: 0.0264 (0.0427)  loss_rpn_box_reg: 0.0183 (0.0229)  time: 0.1453  data: 0.0045  max mem: 1885\n",
            "Epoch: [2]  [100/200]  eta: 0:00:14  lr: 0.000010  loss: 0.1661 (0.1896)  loss_classifier: 0.0629 (0.0700)  loss_box_reg: 0.0512 (0.0540)  loss_objectness: 0.0264 (0.0436)  loss_rpn_box_reg: 0.0095 (0.0219)  time: 0.1463  data: 0.0046  max mem: 1885\n",
            "Epoch: [2]  [110/200]  eta: 0:00:13  lr: 0.000010  loss: 0.1583 (0.1870)  loss_classifier: 0.0601 (0.0694)  loss_box_reg: 0.0457 (0.0532)  loss_objectness: 0.0348 (0.0433)  loss_rpn_box_reg: 0.0093 (0.0211)  time: 0.1469  data: 0.0045  max mem: 1885\n",
            "Epoch: [2]  [120/200]  eta: 0:00:11  lr: 0.000010  loss: 0.1755 (0.1872)  loss_classifier: 0.0668 (0.0697)  loss_box_reg: 0.0568 (0.0543)  loss_objectness: 0.0354 (0.0430)  loss_rpn_box_reg: 0.0093 (0.0202)  time: 0.1468  data: 0.0044  max mem: 1885\n",
            "Epoch: [2]  [130/200]  eta: 0:00:10  lr: 0.000010  loss: 0.1779 (0.1859)  loss_classifier: 0.0587 (0.0686)  loss_box_reg: 0.0447 (0.0543)  loss_objectness: 0.0271 (0.0421)  loss_rpn_box_reg: 0.0109 (0.0208)  time: 0.1465  data: 0.0043  max mem: 1885\n",
            "Epoch: [2]  [140/200]  eta: 0:00:08  lr: 0.000010  loss: 0.1500 (0.1827)  loss_classifier: 0.0505 (0.0673)  loss_box_reg: 0.0363 (0.0533)  loss_objectness: 0.0271 (0.0414)  loss_rpn_box_reg: 0.0166 (0.0208)  time: 0.1463  data: 0.0044  max mem: 1885\n",
            "Epoch: [2]  [150/200]  eta: 0:00:07  lr: 0.000010  loss: 0.1449 (0.1815)  loss_classifier: 0.0506 (0.0672)  loss_box_reg: 0.0363 (0.0529)  loss_objectness: 0.0317 (0.0412)  loss_rpn_box_reg: 0.0121 (0.0202)  time: 0.1465  data: 0.0044  max mem: 1885\n",
            "Epoch: [2]  [160/200]  eta: 0:00:05  lr: 0.000010  loss: 0.1549 (0.1821)  loss_classifier: 0.0570 (0.0675)  loss_box_reg: 0.0363 (0.0526)  loss_objectness: 0.0374 (0.0421)  loss_rpn_box_reg: 0.0112 (0.0200)  time: 0.1463  data: 0.0044  max mem: 1885\n",
            "Epoch: [2]  [170/200]  eta: 0:00:04  lr: 0.000010  loss: 0.1624 (0.1813)  loss_classifier: 0.0570 (0.0670)  loss_box_reg: 0.0428 (0.0526)  loss_objectness: 0.0296 (0.0416)  loss_rpn_box_reg: 0.0136 (0.0201)  time: 0.1466  data: 0.0046  max mem: 1885\n",
            "Epoch: [2]  [180/200]  eta: 0:00:02  lr: 0.000010  loss: 0.1624 (0.1815)  loss_classifier: 0.0510 (0.0661)  loss_box_reg: 0.0427 (0.0521)  loss_objectness: 0.0287 (0.0426)  loss_rpn_box_reg: 0.0173 (0.0207)  time: 0.1465  data: 0.0048  max mem: 1885\n",
            "Epoch: [2]  [190/200]  eta: 0:00:01  lr: 0.000010  loss: 0.1624 (0.1805)  loss_classifier: 0.0510 (0.0660)  loss_box_reg: 0.0447 (0.0523)  loss_objectness: 0.0267 (0.0414)  loss_rpn_box_reg: 0.0177 (0.0207)  time: 0.1468  data: 0.0047  max mem: 1885\n",
            "Epoch: [2]  [199/200]  eta: 0:00:00  lr: 0.000010  loss: 0.1499 (0.1818)  loss_classifier: 0.0544 (0.0658)  loss_box_reg: 0.0426 (0.0518)  loss_objectness: 0.0256 (0.0434)  loss_rpn_box_reg: 0.0146 (0.0207)  time: 0.1469  data: 0.0045  max mem: 1885\n",
            "Epoch: [2] Total time: 0:00:29 (0.1478 s / it)\n",
            "Epoch: [3]  [  0/200]  eta: 0:01:06  lr: 0.000001  loss: 0.2207 (0.2207)  loss_classifier: 0.0900 (0.0900)  loss_box_reg: 0.0487 (0.0487)  loss_objectness: 0.0450 (0.0450)  loss_rpn_box_reg: 0.0370 (0.0370)  time: 0.3305  data: 0.1631  max mem: 1885\n",
            "Epoch: [3]  [ 10/200]  eta: 0:00:30  lr: 0.000001  loss: 0.1843 (0.1936)  loss_classifier: 0.0691 (0.0802)  loss_box_reg: 0.0491 (0.0585)  loss_objectness: 0.0280 (0.0326)  loss_rpn_box_reg: 0.0215 (0.0223)  time: 0.1629  data: 0.0191  max mem: 1885\n",
            "Epoch: [3]  [ 20/200]  eta: 0:00:27  lr: 0.000001  loss: 0.1818 (0.1883)  loss_classifier: 0.0669 (0.0841)  loss_box_reg: 0.0490 (0.0543)  loss_objectness: 0.0280 (0.0319)  loss_rpn_box_reg: 0.0149 (0.0180)  time: 0.1462  data: 0.0046  max mem: 1885\n",
            "Epoch: [3]  [ 30/200]  eta: 0:00:25  lr: 0.000001  loss: 0.1351 (0.1741)  loss_classifier: 0.0552 (0.0773)  loss_box_reg: 0.0402 (0.0493)  loss_objectness: 0.0216 (0.0315)  loss_rpn_box_reg: 0.0098 (0.0159)  time: 0.1468  data: 0.0045  max mem: 1885\n",
            "Epoch: [3]  [ 40/200]  eta: 0:00:24  lr: 0.000001  loss: 0.1529 (0.1736)  loss_classifier: 0.0595 (0.0746)  loss_box_reg: 0.0458 (0.0509)  loss_objectness: 0.0204 (0.0323)  loss_rpn_box_reg: 0.0112 (0.0157)  time: 0.1482  data: 0.0045  max mem: 1885\n",
            "Epoch: [3]  [ 50/200]  eta: 0:00:22  lr: 0.000001  loss: 0.1602 (0.1719)  loss_classifier: 0.0572 (0.0707)  loss_box_reg: 0.0561 (0.0507)  loss_objectness: 0.0197 (0.0348)  loss_rpn_box_reg: 0.0112 (0.0158)  time: 0.1479  data: 0.0046  max mem: 1885\n",
            "Epoch: [3]  [ 60/200]  eta: 0:00:21  lr: 0.000001  loss: 0.1559 (0.1718)  loss_classifier: 0.0572 (0.0708)  loss_box_reg: 0.0493 (0.0509)  loss_objectness: 0.0181 (0.0335)  loss_rpn_box_reg: 0.0084 (0.0165)  time: 0.1469  data: 0.0047  max mem: 1885\n",
            "Epoch: [3]  [ 70/200]  eta: 0:00:19  lr: 0.000001  loss: 0.1559 (0.1715)  loss_classifier: 0.0603 (0.0714)  loss_box_reg: 0.0474 (0.0520)  loss_objectness: 0.0206 (0.0322)  loss_rpn_box_reg: 0.0076 (0.0159)  time: 0.1467  data: 0.0045  max mem: 1885\n",
            "Epoch: [3]  [ 80/200]  eta: 0:00:17  lr: 0.000001  loss: 0.1603 (0.1710)  loss_classifier: 0.0624 (0.0707)  loss_box_reg: 0.0482 (0.0523)  loss_objectness: 0.0233 (0.0320)  loss_rpn_box_reg: 0.0097 (0.0161)  time: 0.1463  data: 0.0043  max mem: 1885\n",
            "Epoch: [3]  [ 90/200]  eta: 0:00:16  lr: 0.000001  loss: 0.1603 (0.1726)  loss_classifier: 0.0594 (0.0699)  loss_box_reg: 0.0471 (0.0524)  loss_objectness: 0.0223 (0.0333)  loss_rpn_box_reg: 0.0164 (0.0170)  time: 0.1464  data: 0.0043  max mem: 1885\n",
            "Epoch: [3]  [100/200]  eta: 0:00:14  lr: 0.000001  loss: 0.1358 (0.1695)  loss_classifier: 0.0534 (0.0683)  loss_box_reg: 0.0435 (0.0512)  loss_objectness: 0.0210 (0.0334)  loss_rpn_box_reg: 0.0097 (0.0166)  time: 0.1464  data: 0.0044  max mem: 1885\n",
            "Epoch: [3]  [110/200]  eta: 0:00:13  lr: 0.000001  loss: 0.1404 (0.1724)  loss_classifier: 0.0474 (0.0688)  loss_box_reg: 0.0418 (0.0520)  loss_objectness: 0.0244 (0.0347)  loss_rpn_box_reg: 0.0095 (0.0169)  time: 0.1463  data: 0.0044  max mem: 1885\n",
            "Epoch: [3]  [120/200]  eta: 0:00:11  lr: 0.000001  loss: 0.1663 (0.1706)  loss_classifier: 0.0583 (0.0679)  loss_box_reg: 0.0546 (0.0514)  loss_objectness: 0.0244 (0.0344)  loss_rpn_box_reg: 0.0105 (0.0170)  time: 0.1467  data: 0.0044  max mem: 1885\n",
            "Epoch: [3]  [130/200]  eta: 0:00:10  lr: 0.000001  loss: 0.1663 (0.1709)  loss_classifier: 0.0573 (0.0677)  loss_box_reg: 0.0520 (0.0515)  loss_objectness: 0.0244 (0.0344)  loss_rpn_box_reg: 0.0098 (0.0173)  time: 0.1467  data: 0.0044  max mem: 1885\n",
            "Epoch: [3]  [140/200]  eta: 0:00:08  lr: 0.000001  loss: 0.1512 (0.1696)  loss_classifier: 0.0613 (0.0671)  loss_box_reg: 0.0542 (0.0514)  loss_objectness: 0.0267 (0.0341)  loss_rpn_box_reg: 0.0085 (0.0168)  time: 0.1470  data: 0.0047  max mem: 1885\n",
            "Epoch: [3]  [150/200]  eta: 0:00:07  lr: 0.000001  loss: 0.1616 (0.1698)  loss_classifier: 0.0641 (0.0673)  loss_box_reg: 0.0549 (0.0523)  loss_objectness: 0.0203 (0.0337)  loss_rpn_box_reg: 0.0090 (0.0164)  time: 0.1472  data: 0.0047  max mem: 1885\n",
            "Epoch: [3]  [160/200]  eta: 0:00:05  lr: 0.000001  loss: 0.1657 (0.1701)  loss_classifier: 0.0629 (0.0676)  loss_box_reg: 0.0626 (0.0531)  loss_objectness: 0.0158 (0.0329)  loss_rpn_box_reg: 0.0098 (0.0164)  time: 0.1473  data: 0.0045  max mem: 1885\n",
            "Epoch: [3]  [170/200]  eta: 0:00:04  lr: 0.000001  loss: 0.1763 (0.1718)  loss_classifier: 0.0629 (0.0677)  loss_box_reg: 0.0598 (0.0538)  loss_objectness: 0.0203 (0.0335)  loss_rpn_box_reg: 0.0134 (0.0169)  time: 0.1476  data: 0.0045  max mem: 1885\n",
            "Epoch: [3]  [180/200]  eta: 0:00:02  lr: 0.000001  loss: 0.1541 (0.1703)  loss_classifier: 0.0576 (0.0670)  loss_box_reg: 0.0490 (0.0536)  loss_objectness: 0.0242 (0.0329)  loss_rpn_box_reg: 0.0138 (0.0169)  time: 0.1469  data: 0.0045  max mem: 1885\n",
            "Epoch: [3]  [190/200]  eta: 0:00:01  lr: 0.000001  loss: 0.1526 (0.1699)  loss_classifier: 0.0523 (0.0666)  loss_box_reg: 0.0436 (0.0538)  loss_objectness: 0.0214 (0.0329)  loss_rpn_box_reg: 0.0112 (0.0165)  time: 0.1463  data: 0.0047  max mem: 1885\n",
            "Epoch: [3]  [199/200]  eta: 0:00:00  lr: 0.000001  loss: 0.1635 (0.1701)  loss_classifier: 0.0639 (0.0666)  loss_box_reg: 0.0567 (0.0544)  loss_objectness: 0.0206 (0.0325)  loss_rpn_box_reg: 0.0118 (0.0165)  time: 0.1462  data: 0.0050  max mem: 1885\n",
            "Epoch: [3] Total time: 0:00:29 (0.1483 s / it)\n",
            "Epoch: [4]  [  0/200]  eta: 0:01:08  lr: 0.000001  loss: 0.2641 (0.2641)  loss_classifier: 0.0977 (0.0977)  loss_box_reg: 0.1352 (0.1352)  loss_objectness: 0.0277 (0.0277)  loss_rpn_box_reg: 0.0035 (0.0035)  time: 0.3425  data: 0.1911  max mem: 1885\n",
            "Epoch: [4]  [ 10/200]  eta: 0:00:31  lr: 0.000001  loss: 0.1336 (0.1645)  loss_classifier: 0.0622 (0.0667)  loss_box_reg: 0.0437 (0.0548)  loss_objectness: 0.0277 (0.0312)  loss_rpn_box_reg: 0.0085 (0.0118)  time: 0.1642  data: 0.0203  max mem: 1885\n",
            "Epoch: [4]  [ 20/200]  eta: 0:00:28  lr: 0.000001  loss: 0.1336 (0.1582)  loss_classifier: 0.0515 (0.0616)  loss_box_reg: 0.0404 (0.0507)  loss_objectness: 0.0246 (0.0302)  loss_rpn_box_reg: 0.0100 (0.0157)  time: 0.1464  data: 0.0040  max mem: 1885\n",
            "Epoch: [4]  [ 30/200]  eta: 0:00:25  lr: 0.000001  loss: 0.1524 (0.1572)  loss_classifier: 0.0497 (0.0596)  loss_box_reg: 0.0472 (0.0519)  loss_objectness: 0.0250 (0.0288)  loss_rpn_box_reg: 0.0132 (0.0168)  time: 0.1463  data: 0.0047  max mem: 1885\n",
            "Epoch: [4]  [ 40/200]  eta: 0:00:24  lr: 0.000001  loss: 0.1524 (0.1578)  loss_classifier: 0.0524 (0.0607)  loss_box_reg: 0.0530 (0.0519)  loss_objectness: 0.0237 (0.0292)  loss_rpn_box_reg: 0.0132 (0.0160)  time: 0.1467  data: 0.0047  max mem: 1885\n",
            "Epoch: [4]  [ 50/200]  eta: 0:00:22  lr: 0.000001  loss: 0.1359 (0.1563)  loss_classifier: 0.0542 (0.0599)  loss_box_reg: 0.0476 (0.0513)  loss_objectness: 0.0112 (0.0291)  loss_rpn_box_reg: 0.0098 (0.0160)  time: 0.1473  data: 0.0046  max mem: 1885\n",
            "Epoch: [4]  [ 60/200]  eta: 0:00:20  lr: 0.000001  loss: 0.1464 (0.1574)  loss_classifier: 0.0542 (0.0604)  loss_box_reg: 0.0476 (0.0510)  loss_objectness: 0.0200 (0.0294)  loss_rpn_box_reg: 0.0138 (0.0165)  time: 0.1468  data: 0.0045  max mem: 1885\n",
            "Epoch: [4]  [ 70/200]  eta: 0:00:19  lr: 0.000001  loss: 0.1579 (0.1568)  loss_classifier: 0.0577 (0.0607)  loss_box_reg: 0.0475 (0.0511)  loss_objectness: 0.0226 (0.0284)  loss_rpn_box_reg: 0.0154 (0.0166)  time: 0.1466  data: 0.0046  max mem: 1885\n",
            "Epoch: [4]  [ 80/200]  eta: 0:00:17  lr: 0.000001  loss: 0.1579 (0.1553)  loss_classifier: 0.0577 (0.0599)  loss_box_reg: 0.0437 (0.0507)  loss_objectness: 0.0201 (0.0277)  loss_rpn_box_reg: 0.0140 (0.0170)  time: 0.1468  data: 0.0046  max mem: 1885\n",
            "Epoch: [4]  [ 90/200]  eta: 0:00:16  lr: 0.000001  loss: 0.1643 (0.1588)  loss_classifier: 0.0635 (0.0619)  loss_box_reg: 0.0609 (0.0521)  loss_objectness: 0.0222 (0.0281)  loss_rpn_box_reg: 0.0130 (0.0167)  time: 0.1468  data: 0.0044  max mem: 1885\n",
            "Epoch: [4]  [100/200]  eta: 0:00:14  lr: 0.000001  loss: 0.1702 (0.1607)  loss_classifier: 0.0648 (0.0624)  loss_box_reg: 0.0615 (0.0534)  loss_objectness: 0.0249 (0.0282)  loss_rpn_box_reg: 0.0123 (0.0167)  time: 0.1465  data: 0.0044  max mem: 1885\n",
            "Epoch: [4]  [110/200]  eta: 0:00:13  lr: 0.000001  loss: 0.1510 (0.1599)  loss_classifier: 0.0561 (0.0618)  loss_box_reg: 0.0527 (0.0526)  loss_objectness: 0.0330 (0.0288)  loss_rpn_box_reg: 0.0116 (0.0167)  time: 0.1463  data: 0.0043  max mem: 1885\n",
            "Epoch: [4]  [120/200]  eta: 0:00:11  lr: 0.000001  loss: 0.1576 (0.1610)  loss_classifier: 0.0558 (0.0626)  loss_box_reg: 0.0476 (0.0532)  loss_objectness: 0.0309 (0.0286)  loss_rpn_box_reg: 0.0116 (0.0165)  time: 0.1472  data: 0.0044  max mem: 1885\n",
            "Epoch: [4]  [130/200]  eta: 0:00:10  lr: 0.000001  loss: 0.1900 (0.1649)  loss_classifier: 0.0603 (0.0630)  loss_box_reg: 0.0518 (0.0543)  loss_objectness: 0.0260 (0.0305)  loss_rpn_box_reg: 0.0129 (0.0171)  time: 0.1474  data: 0.0045  max mem: 1885\n",
            "Epoch: [4]  [140/200]  eta: 0:00:08  lr: 0.000001  loss: 0.1676 (0.1660)  loss_classifier: 0.0597 (0.0634)  loss_box_reg: 0.0518 (0.0550)  loss_objectness: 0.0262 (0.0307)  loss_rpn_box_reg: 0.0097 (0.0170)  time: 0.1462  data: 0.0044  max mem: 1885\n",
            "Epoch: [4]  [150/200]  eta: 0:00:07  lr: 0.000001  loss: 0.1569 (0.1657)  loss_classifier: 0.0560 (0.0635)  loss_box_reg: 0.0519 (0.0549)  loss_objectness: 0.0239 (0.0305)  loss_rpn_box_reg: 0.0097 (0.0167)  time: 0.1466  data: 0.0046  max mem: 1885\n",
            "Epoch: [4]  [160/200]  eta: 0:00:05  lr: 0.000001  loss: 0.1691 (0.1673)  loss_classifier: 0.0645 (0.0640)  loss_box_reg: 0.0579 (0.0553)  loss_objectness: 0.0253 (0.0316)  loss_rpn_box_reg: 0.0087 (0.0164)  time: 0.1471  data: 0.0048  max mem: 1885\n",
            "Epoch: [4]  [170/200]  eta: 0:00:04  lr: 0.000001  loss: 0.1480 (0.1663)  loss_classifier: 0.0564 (0.0635)  loss_box_reg: 0.0464 (0.0548)  loss_objectness: 0.0289 (0.0316)  loss_rpn_box_reg: 0.0095 (0.0165)  time: 0.1465  data: 0.0047  max mem: 1885\n",
            "Epoch: [4]  [180/200]  eta: 0:00:02  lr: 0.000001  loss: 0.1437 (0.1672)  loss_classifier: 0.0558 (0.0638)  loss_box_reg: 0.0464 (0.0554)  loss_objectness: 0.0268 (0.0316)  loss_rpn_box_reg: 0.0130 (0.0163)  time: 0.1460  data: 0.0045  max mem: 1885\n",
            "Epoch: [4]  [190/200]  eta: 0:00:01  lr: 0.000001  loss: 0.1559 (0.1678)  loss_classifier: 0.0584 (0.0638)  loss_box_reg: 0.0528 (0.0555)  loss_objectness: 0.0224 (0.0320)  loss_rpn_box_reg: 0.0106 (0.0164)  time: 0.1471  data: 0.0047  max mem: 1885\n",
            "Epoch: [4]  [199/200]  eta: 0:00:00  lr: 0.000001  loss: 0.1489 (0.1675)  loss_classifier: 0.0520 (0.0641)  loss_box_reg: 0.0437 (0.0552)  loss_objectness: 0.0224 (0.0320)  loss_rpn_box_reg: 0.0113 (0.0162)  time: 0.1478  data: 0.0048  max mem: 1885\n",
            "Epoch: [4] Total time: 0:00:29 (0.1483 s / it)\n",
            "Epoch: [5]  [  0/200]  eta: 0:01:06  lr: 0.000001  loss: 0.1507 (0.1507)  loss_classifier: 0.0506 (0.0506)  loss_box_reg: 0.0585 (0.0585)  loss_objectness: 0.0287 (0.0287)  loss_rpn_box_reg: 0.0129 (0.0129)  time: 0.3309  data: 0.1666  max mem: 1885\n",
            "Epoch: [5]  [ 10/200]  eta: 0:00:31  lr: 0.000001  loss: 0.1507 (0.1574)  loss_classifier: 0.0536 (0.0628)  loss_box_reg: 0.0490 (0.0494)  loss_objectness: 0.0258 (0.0313)  loss_rpn_box_reg: 0.0122 (0.0138)  time: 0.1642  data: 0.0192  max mem: 1885\n",
            "Epoch: [5]  [ 20/200]  eta: 0:00:28  lr: 0.000001  loss: 0.1556 (0.1658)  loss_classifier: 0.0627 (0.0644)  loss_box_reg: 0.0440 (0.0480)  loss_objectness: 0.0244 (0.0376)  loss_rpn_box_reg: 0.0098 (0.0157)  time: 0.1482  data: 0.0046  max mem: 1885\n",
            "Epoch: [5]  [ 30/200]  eta: 0:00:26  lr: 0.000001  loss: 0.1560 (0.1638)  loss_classifier: 0.0613 (0.0624)  loss_box_reg: 0.0483 (0.0481)  loss_objectness: 0.0244 (0.0363)  loss_rpn_box_reg: 0.0111 (0.0170)  time: 0.1487  data: 0.0052  max mem: 1885\n",
            "Epoch: [5]  [ 40/200]  eta: 0:00:24  lr: 0.000001  loss: 0.1568 (0.1641)  loss_classifier: 0.0613 (0.0631)  loss_box_reg: 0.0483 (0.0502)  loss_objectness: 0.0238 (0.0338)  loss_rpn_box_reg: 0.0111 (0.0170)  time: 0.1490  data: 0.0054  max mem: 1885\n",
            "Epoch: [5]  [ 50/200]  eta: 0:00:22  lr: 0.000001  loss: 0.1493 (0.1644)  loss_classifier: 0.0644 (0.0647)  loss_box_reg: 0.0479 (0.0513)  loss_objectness: 0.0238 (0.0327)  loss_rpn_box_reg: 0.0099 (0.0157)  time: 0.1488  data: 0.0051  max mem: 1885\n",
            "Epoch: [5]  [ 60/200]  eta: 0:00:21  lr: 0.000001  loss: 0.1659 (0.1658)  loss_classifier: 0.0625 (0.0650)  loss_box_reg: 0.0530 (0.0525)  loss_objectness: 0.0196 (0.0317)  loss_rpn_box_reg: 0.0130 (0.0167)  time: 0.1483  data: 0.0051  max mem: 1885\n",
            "Epoch: [5]  [ 70/200]  eta: 0:00:19  lr: 0.000001  loss: 0.1511 (0.1655)  loss_classifier: 0.0617 (0.0651)  loss_box_reg: 0.0571 (0.0523)  loss_objectness: 0.0191 (0.0310)  loss_rpn_box_reg: 0.0181 (0.0172)  time: 0.1474  data: 0.0049  max mem: 1885\n",
            "Epoch: [5]  [ 80/200]  eta: 0:00:18  lr: 0.000001  loss: 0.1522 (0.1661)  loss_classifier: 0.0547 (0.0650)  loss_box_reg: 0.0496 (0.0528)  loss_objectness: 0.0202 (0.0316)  loss_rpn_box_reg: 0.0120 (0.0168)  time: 0.1471  data: 0.0043  max mem: 1885\n",
            "Epoch: [5]  [ 90/200]  eta: 0:00:16  lr: 0.000001  loss: 0.1613 (0.1663)  loss_classifier: 0.0586 (0.0650)  loss_box_reg: 0.0447 (0.0529)  loss_objectness: 0.0215 (0.0317)  loss_rpn_box_reg: 0.0101 (0.0167)  time: 0.1473  data: 0.0044  max mem: 1885\n",
            "Epoch: [5]  [100/200]  eta: 0:00:14  lr: 0.000001  loss: 0.1463 (0.1661)  loss_classifier: 0.0610 (0.0649)  loss_box_reg: 0.0470 (0.0528)  loss_objectness: 0.0227 (0.0318)  loss_rpn_box_reg: 0.0093 (0.0165)  time: 0.1468  data: 0.0045  max mem: 1885\n",
            "Epoch: [5]  [110/200]  eta: 0:00:13  lr: 0.000001  loss: 0.1435 (0.1650)  loss_classifier: 0.0574 (0.0644)  loss_box_reg: 0.0424 (0.0521)  loss_objectness: 0.0254 (0.0321)  loss_rpn_box_reg: 0.0083 (0.0165)  time: 0.1464  data: 0.0045  max mem: 1885\n",
            "Epoch: [5]  [120/200]  eta: 0:00:11  lr: 0.000001  loss: 0.1427 (0.1645)  loss_classifier: 0.0558 (0.0642)  loss_box_reg: 0.0437 (0.0524)  loss_objectness: 0.0247 (0.0316)  loss_rpn_box_reg: 0.0098 (0.0163)  time: 0.1466  data: 0.0045  max mem: 1885\n",
            "Epoch: [5]  [130/200]  eta: 0:00:10  lr: 0.000001  loss: 0.1466 (0.1660)  loss_classifier: 0.0615 (0.0645)  loss_box_reg: 0.0512 (0.0528)  loss_objectness: 0.0218 (0.0319)  loss_rpn_box_reg: 0.0150 (0.0167)  time: 0.1472  data: 0.0045  max mem: 1885\n",
            "Epoch: [5]  [140/200]  eta: 0:00:08  lr: 0.000001  loss: 0.1626 (0.1657)  loss_classifier: 0.0615 (0.0643)  loss_box_reg: 0.0512 (0.0531)  loss_objectness: 0.0229 (0.0315)  loss_rpn_box_reg: 0.0110 (0.0168)  time: 0.1471  data: 0.0044  max mem: 1885\n",
            "Epoch: [5]  [150/200]  eta: 0:00:07  lr: 0.000001  loss: 0.1728 (0.1681)  loss_classifier: 0.0615 (0.0650)  loss_box_reg: 0.0553 (0.0539)  loss_objectness: 0.0256 (0.0322)  loss_rpn_box_reg: 0.0158 (0.0170)  time: 0.1467  data: 0.0043  max mem: 1885\n",
            "Epoch: [5]  [160/200]  eta: 0:00:05  lr: 0.000001  loss: 0.1725 (0.1685)  loss_classifier: 0.0649 (0.0652)  loss_box_reg: 0.0479 (0.0542)  loss_objectness: 0.0256 (0.0321)  loss_rpn_box_reg: 0.0115 (0.0170)  time: 0.1462  data: 0.0044  max mem: 1885\n",
            "Epoch: [5]  [170/200]  eta: 0:00:04  lr: 0.000001  loss: 0.1469 (0.1679)  loss_classifier: 0.0588 (0.0651)  loss_box_reg: 0.0452 (0.0539)  loss_objectness: 0.0242 (0.0321)  loss_rpn_box_reg: 0.0112 (0.0169)  time: 0.1467  data: 0.0047  max mem: 1885\n",
            "Epoch: [5]  [180/200]  eta: 0:00:02  lr: 0.000001  loss: 0.1409 (0.1676)  loss_classifier: 0.0523 (0.0649)  loss_box_reg: 0.0504 (0.0545)  loss_objectness: 0.0202 (0.0315)  loss_rpn_box_reg: 0.0099 (0.0167)  time: 0.1468  data: 0.0047  max mem: 1885\n",
            "Epoch: [5]  [190/200]  eta: 0:00:01  lr: 0.000001  loss: 0.1579 (0.1681)  loss_classifier: 0.0611 (0.0656)  loss_box_reg: 0.0611 (0.0551)  loss_objectness: 0.0187 (0.0309)  loss_rpn_box_reg: 0.0099 (0.0166)  time: 0.1464  data: 0.0044  max mem: 1885\n",
            "Epoch: [5]  [199/200]  eta: 0:00:00  lr: 0.000001  loss: 0.1595 (0.1686)  loss_classifier: 0.0702 (0.0660)  loss_box_reg: 0.0556 (0.0550)  loss_objectness: 0.0192 (0.0313)  loss_rpn_box_reg: 0.0086 (0.0163)  time: 0.1462  data: 0.0045  max mem: 1885\n",
            "Epoch: [5] Total time: 0:00:29 (0.1487 s / it)\n",
            "Epoch: [6]  [  0/200]  eta: 0:01:12  lr: 0.000000  loss: 0.1786 (0.1786)  loss_classifier: 0.0821 (0.0821)  loss_box_reg: 0.0850 (0.0850)  loss_objectness: 0.0041 (0.0041)  loss_rpn_box_reg: 0.0074 (0.0074)  time: 0.3607  data: 0.2054  max mem: 1885\n",
            "Epoch: [6]  [ 10/200]  eta: 0:00:31  lr: 0.000000  loss: 0.1716 (0.1799)  loss_classifier: 0.0669 (0.0720)  loss_box_reg: 0.0450 (0.0587)  loss_objectness: 0.0289 (0.0332)  loss_rpn_box_reg: 0.0092 (0.0161)  time: 0.1640  data: 0.0204  max mem: 1885\n",
            "Epoch: [6]  [ 20/200]  eta: 0:00:28  lr: 0.000000  loss: 0.1716 (0.1772)  loss_classifier: 0.0601 (0.0673)  loss_box_reg: 0.0479 (0.0559)  loss_objectness: 0.0260 (0.0331)  loss_rpn_box_reg: 0.0158 (0.0209)  time: 0.1455  data: 0.0032  max mem: 1885\n",
            "Epoch: [6]  [ 30/200]  eta: 0:00:25  lr: 0.000000  loss: 0.1727 (0.1726)  loss_classifier: 0.0597 (0.0674)  loss_box_reg: 0.0528 (0.0552)  loss_objectness: 0.0245 (0.0320)  loss_rpn_box_reg: 0.0124 (0.0179)  time: 0.1467  data: 0.0044  max mem: 1885\n",
            "Epoch: [6]  [ 40/200]  eta: 0:00:24  lr: 0.000000  loss: 0.1656 (0.1737)  loss_classifier: 0.0674 (0.0689)  loss_box_reg: 0.0618 (0.0584)  loss_objectness: 0.0216 (0.0301)  loss_rpn_box_reg: 0.0095 (0.0163)  time: 0.1479  data: 0.0046  max mem: 1885\n",
            "Epoch: [6]  [ 50/200]  eta: 0:00:22  lr: 0.000000  loss: 0.1533 (0.1708)  loss_classifier: 0.0674 (0.0677)  loss_box_reg: 0.0531 (0.0560)  loss_objectness: 0.0231 (0.0292)  loss_rpn_box_reg: 0.0103 (0.0178)  time: 0.1474  data: 0.0046  max mem: 1885\n",
            "Epoch: [6]  [ 60/200]  eta: 0:00:21  lr: 0.000000  loss: 0.1533 (0.1714)  loss_classifier: 0.0612 (0.0686)  loss_box_reg: 0.0430 (0.0567)  loss_objectness: 0.0233 (0.0288)  loss_rpn_box_reg: 0.0103 (0.0173)  time: 0.1464  data: 0.0047  max mem: 1885\n",
            "Epoch: [6]  [ 70/200]  eta: 0:00:19  lr: 0.000000  loss: 0.1556 (0.1689)  loss_classifier: 0.0618 (0.0675)  loss_box_reg: 0.0505 (0.0546)  loss_objectness: 0.0214 (0.0299)  loss_rpn_box_reg: 0.0077 (0.0169)  time: 0.1476  data: 0.0050  max mem: 1885\n",
            "Epoch: [6]  [ 80/200]  eta: 0:00:17  lr: 0.000000  loss: 0.1556 (0.1687)  loss_classifier: 0.0644 (0.0675)  loss_box_reg: 0.0505 (0.0555)  loss_objectness: 0.0197 (0.0289)  loss_rpn_box_reg: 0.0114 (0.0168)  time: 0.1479  data: 0.0049  max mem: 1885\n",
            "Epoch: [6]  [ 90/200]  eta: 0:00:16  lr: 0.000000  loss: 0.1759 (0.1696)  loss_classifier: 0.0644 (0.0672)  loss_box_reg: 0.0522 (0.0555)  loss_objectness: 0.0239 (0.0299)  loss_rpn_box_reg: 0.0128 (0.0171)  time: 0.1475  data: 0.0045  max mem: 1885\n",
            "Epoch: [6]  [100/200]  eta: 0:00:14  lr: 0.000000  loss: 0.1627 (0.1699)  loss_classifier: 0.0558 (0.0668)  loss_box_reg: 0.0436 (0.0558)  loss_objectness: 0.0262 (0.0307)  loss_rpn_box_reg: 0.0118 (0.0167)  time: 0.1476  data: 0.0044  max mem: 1885\n",
            "Epoch: [6]  [110/200]  eta: 0:00:13  lr: 0.000000  loss: 0.1593 (0.1694)  loss_classifier: 0.0535 (0.0668)  loss_box_reg: 0.0436 (0.0559)  loss_objectness: 0.0221 (0.0298)  loss_rpn_box_reg: 0.0139 (0.0170)  time: 0.1472  data: 0.0045  max mem: 1885\n",
            "Epoch: [6]  [120/200]  eta: 0:00:11  lr: 0.000000  loss: 0.1490 (0.1690)  loss_classifier: 0.0601 (0.0660)  loss_box_reg: 0.0426 (0.0556)  loss_objectness: 0.0223 (0.0299)  loss_rpn_box_reg: 0.0202 (0.0175)  time: 0.1465  data: 0.0047  max mem: 1885\n",
            "Epoch: [6]  [130/200]  eta: 0:00:10  lr: 0.000000  loss: 0.1504 (0.1689)  loss_classifier: 0.0601 (0.0658)  loss_box_reg: 0.0457 (0.0554)  loss_objectness: 0.0267 (0.0306)  loss_rpn_box_reg: 0.0122 (0.0171)  time: 0.1464  data: 0.0046  max mem: 1885\n",
            "Epoch: [6]  [140/200]  eta: 0:00:08  lr: 0.000000  loss: 0.1631 (0.1694)  loss_classifier: 0.0552 (0.0661)  loss_box_reg: 0.0592 (0.0558)  loss_objectness: 0.0195 (0.0304)  loss_rpn_box_reg: 0.0099 (0.0170)  time: 0.1462  data: 0.0044  max mem: 1885\n",
            "Epoch: [6]  [150/200]  eta: 0:00:07  lr: 0.000000  loss: 0.1639 (0.1708)  loss_classifier: 0.0678 (0.0672)  loss_box_reg: 0.0585 (0.0561)  loss_objectness: 0.0195 (0.0308)  loss_rpn_box_reg: 0.0112 (0.0167)  time: 0.1463  data: 0.0045  max mem: 1885\n",
            "Epoch: [6]  [160/200]  eta: 0:00:05  lr: 0.000000  loss: 0.1565 (0.1693)  loss_classifier: 0.0639 (0.0666)  loss_box_reg: 0.0452 (0.0553)  loss_objectness: 0.0249 (0.0308)  loss_rpn_box_reg: 0.0104 (0.0166)  time: 0.1465  data: 0.0045  max mem: 1885\n",
            "Epoch: [6]  [170/200]  eta: 0:00:04  lr: 0.000000  loss: 0.1406 (0.1669)  loss_classifier: 0.0565 (0.0658)  loss_box_reg: 0.0425 (0.0545)  loss_objectness: 0.0200 (0.0303)  loss_rpn_box_reg: 0.0083 (0.0163)  time: 0.1467  data: 0.0044  max mem: 1885\n",
            "Epoch: [6]  [180/200]  eta: 0:00:02  lr: 0.000000  loss: 0.1400 (0.1674)  loss_classifier: 0.0565 (0.0659)  loss_box_reg: 0.0458 (0.0551)  loss_objectness: 0.0178 (0.0303)  loss_rpn_box_reg: 0.0077 (0.0162)  time: 0.1472  data: 0.0045  max mem: 1885\n",
            "Epoch: [6]  [190/200]  eta: 0:00:01  lr: 0.000000  loss: 0.1456 (0.1672)  loss_classifier: 0.0584 (0.0657)  loss_box_reg: 0.0526 (0.0547)  loss_objectness: 0.0183 (0.0305)  loss_rpn_box_reg: 0.0124 (0.0163)  time: 0.1468  data: 0.0044  max mem: 1885\n",
            "Epoch: [6]  [199/200]  eta: 0:00:00  lr: 0.000000  loss: 0.1426 (0.1666)  loss_classifier: 0.0558 (0.0654)  loss_box_reg: 0.0464 (0.0544)  loss_objectness: 0.0250 (0.0306)  loss_rpn_box_reg: 0.0097 (0.0161)  time: 0.1454  data: 0.0043  max mem: 1885\n",
            "Epoch: [6] Total time: 0:00:29 (0.1484 s / it)\n",
            "Epoch: [7]  [  0/200]  eta: 0:01:10  lr: 0.000000  loss: 0.2229 (0.2229)  loss_classifier: 0.1050 (0.1050)  loss_box_reg: 0.0902 (0.0902)  loss_objectness: 0.0186 (0.0186)  loss_rpn_box_reg: 0.0090 (0.0090)  time: 0.3526  data: 0.1983  max mem: 1885\n",
            "Epoch: [7]  [ 10/200]  eta: 0:00:31  lr: 0.000000  loss: 0.1450 (0.1574)  loss_classifier: 0.0440 (0.0582)  loss_box_reg: 0.0427 (0.0526)  loss_objectness: 0.0190 (0.0244)  loss_rpn_box_reg: 0.0110 (0.0222)  time: 0.1641  data: 0.0201  max mem: 1885\n",
            "Epoch: [7]  [ 20/200]  eta: 0:00:28  lr: 0.000000  loss: 0.1450 (0.1634)  loss_classifier: 0.0532 (0.0628)  loss_box_reg: 0.0411 (0.0489)  loss_objectness: 0.0216 (0.0337)  loss_rpn_box_reg: 0.0100 (0.0180)  time: 0.1457  data: 0.0034  max mem: 1885\n",
            "Epoch: [7]  [ 30/200]  eta: 0:00:25  lr: 0.000000  loss: 0.1621 (0.1697)  loss_classifier: 0.0641 (0.0673)  loss_box_reg: 0.0551 (0.0553)  loss_objectness: 0.0201 (0.0289)  loss_rpn_box_reg: 0.0104 (0.0182)  time: 0.1463  data: 0.0044  max mem: 1885\n",
            "Epoch: [7]  [ 40/200]  eta: 0:00:24  lr: 0.000000  loss: 0.1754 (0.1754)  loss_classifier: 0.0707 (0.0701)  loss_box_reg: 0.0596 (0.0587)  loss_objectness: 0.0206 (0.0304)  loss_rpn_box_reg: 0.0104 (0.0162)  time: 0.1464  data: 0.0044  max mem: 1885\n",
            "Epoch: [7]  [ 50/200]  eta: 0:00:22  lr: 0.000000  loss: 0.1688 (0.1695)  loss_classifier: 0.0681 (0.0673)  loss_box_reg: 0.0460 (0.0550)  loss_objectness: 0.0296 (0.0324)  loss_rpn_box_reg: 0.0052 (0.0149)  time: 0.1464  data: 0.0044  max mem: 1885\n",
            "Epoch: [7]  [ 60/200]  eta: 0:00:20  lr: 0.000000  loss: 0.1548 (0.1694)  loss_classifier: 0.0611 (0.0667)  loss_box_reg: 0.0460 (0.0558)  loss_objectness: 0.0274 (0.0322)  loss_rpn_box_reg: 0.0081 (0.0146)  time: 0.1467  data: 0.0044  max mem: 1885\n",
            "Epoch: [7]  [ 70/200]  eta: 0:00:19  lr: 0.000000  loss: 0.1481 (0.1657)  loss_classifier: 0.0611 (0.0653)  loss_box_reg: 0.0565 (0.0551)  loss_objectness: 0.0213 (0.0304)  loss_rpn_box_reg: 0.0109 (0.0150)  time: 0.1471  data: 0.0044  max mem: 1885\n",
            "Epoch: [7]  [ 80/200]  eta: 0:00:17  lr: 0.000000  loss: 0.1406 (0.1671)  loss_classifier: 0.0524 (0.0647)  loss_box_reg: 0.0442 (0.0536)  loss_objectness: 0.0226 (0.0325)  loss_rpn_box_reg: 0.0136 (0.0163)  time: 0.1463  data: 0.0044  max mem: 1885\n",
            "Epoch: [7]  [ 90/200]  eta: 0:00:16  lr: 0.000000  loss: 0.1902 (0.1711)  loss_classifier: 0.0692 (0.0673)  loss_box_reg: 0.0482 (0.0559)  loss_objectness: 0.0237 (0.0320)  loss_rpn_box_reg: 0.0124 (0.0159)  time: 0.1463  data: 0.0046  max mem: 1885\n",
            "Epoch: [7]  [100/200]  eta: 0:00:14  lr: 0.000000  loss: 0.1651 (0.1695)  loss_classifier: 0.0733 (0.0669)  loss_box_reg: 0.0598 (0.0558)  loss_objectness: 0.0233 (0.0315)  loss_rpn_box_reg: 0.0100 (0.0152)  time: 0.1466  data: 0.0046  max mem: 1885\n",
            "Epoch: [7]  [110/200]  eta: 0:00:13  lr: 0.000000  loss: 0.1465 (0.1684)  loss_classifier: 0.0576 (0.0664)  loss_box_reg: 0.0512 (0.0559)  loss_objectness: 0.0218 (0.0303)  loss_rpn_box_reg: 0.0100 (0.0157)  time: 0.1466  data: 0.0043  max mem: 1885\n",
            "Epoch: [7]  [120/200]  eta: 0:00:11  lr: 0.000000  loss: 0.1565 (0.1690)  loss_classifier: 0.0537 (0.0662)  loss_box_reg: 0.0492 (0.0560)  loss_objectness: 0.0202 (0.0308)  loss_rpn_box_reg: 0.0133 (0.0161)  time: 0.1474  data: 0.0043  max mem: 1885\n",
            "Epoch: [7]  [130/200]  eta: 0:00:10  lr: 0.000000  loss: 0.1727 (0.1701)  loss_classifier: 0.0584 (0.0662)  loss_box_reg: 0.0536 (0.0559)  loss_objectness: 0.0270 (0.0318)  loss_rpn_box_reg: 0.0136 (0.0162)  time: 0.1468  data: 0.0044  max mem: 1885\n",
            "Epoch: [7]  [140/200]  eta: 0:00:08  lr: 0.000000  loss: 0.1616 (0.1694)  loss_classifier: 0.0584 (0.0661)  loss_box_reg: 0.0439 (0.0551)  loss_objectness: 0.0272 (0.0322)  loss_rpn_box_reg: 0.0107 (0.0160)  time: 0.1462  data: 0.0044  max mem: 1885\n",
            "Epoch: [7]  [150/200]  eta: 0:00:07  lr: 0.000000  loss: 0.1481 (0.1688)  loss_classifier: 0.0629 (0.0661)  loss_box_reg: 0.0430 (0.0551)  loss_objectness: 0.0243 (0.0317)  loss_rpn_box_reg: 0.0112 (0.0158)  time: 0.1464  data: 0.0045  max mem: 1885\n",
            "Epoch: [7]  [160/200]  eta: 0:00:05  lr: 0.000000  loss: 0.1423 (0.1682)  loss_classifier: 0.0567 (0.0657)  loss_box_reg: 0.0349 (0.0545)  loss_objectness: 0.0298 (0.0319)  loss_rpn_box_reg: 0.0114 (0.0162)  time: 0.1462  data: 0.0044  max mem: 1885\n",
            "Epoch: [7]  [170/200]  eta: 0:00:04  lr: 0.000000  loss: 0.1406 (0.1679)  loss_classifier: 0.0545 (0.0659)  loss_box_reg: 0.0416 (0.0548)  loss_objectness: 0.0259 (0.0316)  loss_rpn_box_reg: 0.0095 (0.0157)  time: 0.1461  data: 0.0044  max mem: 1885\n",
            "Epoch: [7]  [180/200]  eta: 0:00:02  lr: 0.000000  loss: 0.1529 (0.1685)  loss_classifier: 0.0572 (0.0663)  loss_box_reg: 0.0624 (0.0555)  loss_objectness: 0.0168 (0.0311)  loss_rpn_box_reg: 0.0112 (0.0156)  time: 0.1468  data: 0.0046  max mem: 1885\n",
            "Epoch: [7]  [190/200]  eta: 0:00:01  lr: 0.000000  loss: 0.1578 (0.1676)  loss_classifier: 0.0571 (0.0659)  loss_box_reg: 0.0578 (0.0550)  loss_objectness: 0.0238 (0.0310)  loss_rpn_box_reg: 0.0100 (0.0156)  time: 0.1465  data: 0.0045  max mem: 1885\n",
            "Epoch: [7]  [199/200]  eta: 0:00:00  lr: 0.000000  loss: 0.1416 (0.1667)  loss_classifier: 0.0487 (0.0652)  loss_box_reg: 0.0407 (0.0545)  loss_objectness: 0.0308 (0.0309)  loss_rpn_box_reg: 0.0135 (0.0161)  time: 0.1449  data: 0.0043  max mem: 1885\n",
            "Epoch: [7] Total time: 0:00:29 (0.1479 s / it)\n",
            "Epoch: [8]  [  0/200]  eta: 0:01:06  lr: 0.000000  loss: 0.1291 (0.1291)  loss_classifier: 0.0436 (0.0436)  loss_box_reg: 0.0378 (0.0378)  loss_objectness: 0.0427 (0.0427)  loss_rpn_box_reg: 0.0049 (0.0049)  time: 0.3333  data: 0.1830  max mem: 1885\n",
            "Epoch: [8]  [ 10/200]  eta: 0:00:30  lr: 0.000000  loss: 0.1395 (0.1382)  loss_classifier: 0.0487 (0.0521)  loss_box_reg: 0.0378 (0.0393)  loss_objectness: 0.0335 (0.0323)  loss_rpn_box_reg: 0.0110 (0.0145)  time: 0.1616  data: 0.0197  max mem: 1885\n",
            "Epoch: [8]  [ 20/200]  eta: 0:00:27  lr: 0.000000  loss: 0.1485 (0.1571)  loss_classifier: 0.0575 (0.0600)  loss_box_reg: 0.0461 (0.0478)  loss_objectness: 0.0259 (0.0332)  loss_rpn_box_reg: 0.0145 (0.0161)  time: 0.1458  data: 0.0041  max mem: 1885\n",
            "Epoch: [8]  [ 30/200]  eta: 0:00:25  lr: 0.000000  loss: 0.1591 (0.1598)  loss_classifier: 0.0662 (0.0633)  loss_box_reg: 0.0573 (0.0509)  loss_objectness: 0.0223 (0.0295)  loss_rpn_box_reg: 0.0126 (0.0161)  time: 0.1464  data: 0.0046  max mem: 1885\n",
            "Epoch: [8]  [ 40/200]  eta: 0:00:24  lr: 0.000000  loss: 0.1576 (0.1582)  loss_classifier: 0.0479 (0.0622)  loss_box_reg: 0.0427 (0.0497)  loss_objectness: 0.0180 (0.0291)  loss_rpn_box_reg: 0.0103 (0.0172)  time: 0.1463  data: 0.0044  max mem: 1885\n",
            "Epoch: [8]  [ 50/200]  eta: 0:00:22  lr: 0.000000  loss: 0.1526 (0.1579)  loss_classifier: 0.0569 (0.0621)  loss_box_reg: 0.0394 (0.0506)  loss_objectness: 0.0165 (0.0282)  loss_rpn_box_reg: 0.0115 (0.0170)  time: 0.1486  data: 0.0047  max mem: 1885\n",
            "Epoch: [8]  [ 60/200]  eta: 0:00:20  lr: 0.000000  loss: 0.1665 (0.1640)  loss_classifier: 0.0666 (0.0654)  loss_box_reg: 0.0551 (0.0539)  loss_objectness: 0.0241 (0.0277)  loss_rpn_box_reg: 0.0099 (0.0170)  time: 0.1484  data: 0.0049  max mem: 1885\n",
            "Epoch: [8]  [ 70/200]  eta: 0:00:19  lr: 0.000000  loss: 0.1584 (0.1642)  loss_classifier: 0.0645 (0.0660)  loss_box_reg: 0.0551 (0.0543)  loss_objectness: 0.0203 (0.0270)  loss_rpn_box_reg: 0.0112 (0.0170)  time: 0.1469  data: 0.0049  max mem: 1885\n",
            "Epoch: [8]  [ 80/200]  eta: 0:00:17  lr: 0.000000  loss: 0.1659 (0.1661)  loss_classifier: 0.0634 (0.0666)  loss_box_reg: 0.0568 (0.0562)  loss_objectness: 0.0196 (0.0266)  loss_rpn_box_reg: 0.0115 (0.0167)  time: 0.1469  data: 0.0048  max mem: 1885\n",
            "Epoch: [8]  [ 90/200]  eta: 0:00:16  lr: 0.000000  loss: 0.1694 (0.1683)  loss_classifier: 0.0704 (0.0677)  loss_box_reg: 0.0640 (0.0572)  loss_objectness: 0.0185 (0.0267)  loss_rpn_box_reg: 0.0127 (0.0166)  time: 0.1465  data: 0.0045  max mem: 1885\n",
            "Epoch: [8]  [100/200]  eta: 0:00:14  lr: 0.000000  loss: 0.1715 (0.1688)  loss_classifier: 0.0783 (0.0681)  loss_box_reg: 0.0529 (0.0571)  loss_objectness: 0.0226 (0.0271)  loss_rpn_box_reg: 0.0115 (0.0164)  time: 0.1463  data: 0.0046  max mem: 1885\n",
            "Epoch: [8]  [110/200]  eta: 0:00:13  lr: 0.000000  loss: 0.1714 (0.1698)  loss_classifier: 0.0631 (0.0677)  loss_box_reg: 0.0503 (0.0569)  loss_objectness: 0.0270 (0.0285)  loss_rpn_box_reg: 0.0112 (0.0167)  time: 0.1469  data: 0.0046  max mem: 1885\n",
            "Epoch: [8]  [120/200]  eta: 0:00:11  lr: 0.000000  loss: 0.1470 (0.1682)  loss_classifier: 0.0599 (0.0667)  loss_box_reg: 0.0426 (0.0559)  loss_objectness: 0.0229 (0.0289)  loss_rpn_box_reg: 0.0091 (0.0166)  time: 0.1475  data: 0.0045  max mem: 1885\n",
            "Epoch: [8]  [130/200]  eta: 0:00:10  lr: 0.000000  loss: 0.1341 (0.1658)  loss_classifier: 0.0523 (0.0656)  loss_box_reg: 0.0384 (0.0548)  loss_objectness: 0.0229 (0.0289)  loss_rpn_box_reg: 0.0071 (0.0164)  time: 0.1466  data: 0.0045  max mem: 1885\n",
            "Epoch: [8]  [140/200]  eta: 0:00:08  lr: 0.000000  loss: 0.1389 (0.1657)  loss_classifier: 0.0523 (0.0653)  loss_box_reg: 0.0383 (0.0538)  loss_objectness: 0.0210 (0.0297)  loss_rpn_box_reg: 0.0110 (0.0169)  time: 0.1462  data: 0.0045  max mem: 1885\n",
            "Epoch: [8]  [150/200]  eta: 0:00:07  lr: 0.000000  loss: 0.1510 (0.1670)  loss_classifier: 0.0606 (0.0663)  loss_box_reg: 0.0405 (0.0543)  loss_objectness: 0.0223 (0.0298)  loss_rpn_box_reg: 0.0117 (0.0165)  time: 0.1465  data: 0.0045  max mem: 1885\n",
            "Epoch: [8]  [160/200]  eta: 0:00:05  lr: 0.000000  loss: 0.1543 (0.1672)  loss_classifier: 0.0588 (0.0660)  loss_box_reg: 0.0429 (0.0543)  loss_objectness: 0.0235 (0.0301)  loss_rpn_box_reg: 0.0106 (0.0167)  time: 0.1467  data: 0.0047  max mem: 1885\n",
            "Epoch: [8]  [170/200]  eta: 0:00:04  lr: 0.000000  loss: 0.1567 (0.1672)  loss_classifier: 0.0570 (0.0666)  loss_box_reg: 0.0455 (0.0546)  loss_objectness: 0.0185 (0.0296)  loss_rpn_box_reg: 0.0112 (0.0164)  time: 0.1470  data: 0.0047  max mem: 1885\n",
            "Epoch: [8]  [180/200]  eta: 0:00:02  lr: 0.000000  loss: 0.1379 (0.1662)  loss_classifier: 0.0570 (0.0658)  loss_box_reg: 0.0465 (0.0542)  loss_objectness: 0.0178 (0.0299)  loss_rpn_box_reg: 0.0106 (0.0163)  time: 0.1471  data: 0.0044  max mem: 1885\n",
            "Epoch: [8]  [190/200]  eta: 0:00:01  lr: 0.000000  loss: 0.1442 (0.1669)  loss_classifier: 0.0548 (0.0660)  loss_box_reg: 0.0505 (0.0545)  loss_objectness: 0.0192 (0.0302)  loss_rpn_box_reg: 0.0106 (0.0162)  time: 0.1469  data: 0.0044  max mem: 1885\n",
            "Epoch: [8]  [199/200]  eta: 0:00:00  lr: 0.000000  loss: 0.1479 (0.1667)  loss_classifier: 0.0525 (0.0656)  loss_box_reg: 0.0548 (0.0548)  loss_objectness: 0.0255 (0.0303)  loss_rpn_box_reg: 0.0092 (0.0160)  time: 0.1461  data: 0.0045  max mem: 1885\n",
            "Epoch: [8] Total time: 0:00:29 (0.1482 s / it)\n",
            "Epoch: [9]  [  0/200]  eta: 0:01:06  lr: 0.000000  loss: 0.1798 (0.1798)  loss_classifier: 0.0687 (0.0687)  loss_box_reg: 0.0887 (0.0887)  loss_objectness: 0.0119 (0.0119)  loss_rpn_box_reg: 0.0105 (0.0105)  time: 0.3309  data: 0.1734  max mem: 1885\n",
            "Epoch: [9]  [ 10/200]  eta: 0:00:31  lr: 0.000000  loss: 0.1608 (0.1515)  loss_classifier: 0.0683 (0.0641)  loss_box_reg: 0.0423 (0.0466)  loss_objectness: 0.0254 (0.0304)  loss_rpn_box_reg: 0.0100 (0.0104)  time: 0.1637  data: 0.0197  max mem: 1885\n",
            "Epoch: [9]  [ 20/200]  eta: 0:00:28  lr: 0.000000  loss: 0.1433 (0.1524)  loss_classifier: 0.0599 (0.0631)  loss_box_reg: 0.0508 (0.0508)  loss_objectness: 0.0219 (0.0266)  loss_rpn_box_reg: 0.0100 (0.0119)  time: 0.1468  data: 0.0044  max mem: 1885\n",
            "Epoch: [9]  [ 30/200]  eta: 0:00:25  lr: 0.000000  loss: 0.1433 (0.1546)  loss_classifier: 0.0609 (0.0638)  loss_box_reg: 0.0545 (0.0528)  loss_objectness: 0.0203 (0.0268)  loss_rpn_box_reg: 0.0077 (0.0112)  time: 0.1469  data: 0.0045  max mem: 1885\n",
            "Epoch: [9]  [ 40/200]  eta: 0:00:24  lr: 0.000000  loss: 0.1562 (0.1595)  loss_classifier: 0.0623 (0.0653)  loss_box_reg: 0.0503 (0.0534)  loss_objectness: 0.0233 (0.0282)  loss_rpn_box_reg: 0.0077 (0.0125)  time: 0.1474  data: 0.0046  max mem: 1885\n",
            "Epoch: [9]  [ 50/200]  eta: 0:00:22  lr: 0.000000  loss: 0.1572 (0.1618)  loss_classifier: 0.0700 (0.0669)  loss_box_reg: 0.0503 (0.0542)  loss_objectness: 0.0233 (0.0280)  loss_rpn_box_reg: 0.0108 (0.0128)  time: 0.1472  data: 0.0046  max mem: 1885\n",
            "Epoch: [9]  [ 60/200]  eta: 0:00:21  lr: 0.000000  loss: 0.1572 (0.1615)  loss_classifier: 0.0727 (0.0676)  loss_box_reg: 0.0572 (0.0541)  loss_objectness: 0.0220 (0.0273)  loss_rpn_box_reg: 0.0096 (0.0126)  time: 0.1471  data: 0.0046  max mem: 1885\n",
            "Epoch: [9]  [ 70/200]  eta: 0:00:19  lr: 0.000000  loss: 0.1456 (0.1576)  loss_classifier: 0.0535 (0.0646)  loss_box_reg: 0.0349 (0.0514)  loss_objectness: 0.0215 (0.0272)  loss_rpn_box_reg: 0.0097 (0.0144)  time: 0.1469  data: 0.0045  max mem: 1885\n",
            "Epoch: [9]  [ 80/200]  eta: 0:00:17  lr: 0.000000  loss: 0.1422 (0.1601)  loss_classifier: 0.0449 (0.0645)  loss_box_reg: 0.0349 (0.0512)  loss_objectness: 0.0248 (0.0293)  loss_rpn_box_reg: 0.0131 (0.0151)  time: 0.1462  data: 0.0044  max mem: 1885\n",
            "Epoch: [9]  [ 90/200]  eta: 0:00:16  lr: 0.000000  loss: 0.1582 (0.1625)  loss_classifier: 0.0566 (0.0647)  loss_box_reg: 0.0411 (0.0514)  loss_objectness: 0.0284 (0.0310)  loss_rpn_box_reg: 0.0125 (0.0153)  time: 0.1461  data: 0.0044  max mem: 1885\n",
            "Epoch: [9]  [100/200]  eta: 0:00:14  lr: 0.000000  loss: 0.1587 (0.1649)  loss_classifier: 0.0571 (0.0645)  loss_box_reg: 0.0479 (0.0522)  loss_objectness: 0.0222 (0.0325)  loss_rpn_box_reg: 0.0089 (0.0157)  time: 0.1466  data: 0.0045  max mem: 1885\n",
            "Epoch: [9]  [110/200]  eta: 0:00:13  lr: 0.000000  loss: 0.1582 (0.1658)  loss_classifier: 0.0607 (0.0657)  loss_box_reg: 0.0607 (0.0531)  loss_objectness: 0.0206 (0.0316)  loss_rpn_box_reg: 0.0115 (0.0154)  time: 0.1466  data: 0.0045  max mem: 1885\n",
            "Epoch: [9]  [120/200]  eta: 0:00:11  lr: 0.000000  loss: 0.1582 (0.1644)  loss_classifier: 0.0607 (0.0653)  loss_box_reg: 0.0607 (0.0531)  loss_objectness: 0.0186 (0.0306)  loss_rpn_box_reg: 0.0112 (0.0154)  time: 0.1463  data: 0.0045  max mem: 1885\n",
            "Epoch: [9]  [130/200]  eta: 0:00:10  lr: 0.000000  loss: 0.1518 (0.1646)  loss_classifier: 0.0566 (0.0651)  loss_box_reg: 0.0490 (0.0533)  loss_objectness: 0.0198 (0.0305)  loss_rpn_box_reg: 0.0112 (0.0156)  time: 0.1468  data: 0.0045  max mem: 1885\n",
            "Epoch: [9]  [140/200]  eta: 0:00:08  lr: 0.000000  loss: 0.1573 (0.1645)  loss_classifier: 0.0547 (0.0644)  loss_box_reg: 0.0499 (0.0533)  loss_objectness: 0.0294 (0.0308)  loss_rpn_box_reg: 0.0159 (0.0161)  time: 0.1474  data: 0.0045  max mem: 1885\n",
            "Epoch: [9]  [150/200]  eta: 0:00:07  lr: 0.000000  loss: 0.1595 (0.1647)  loss_classifier: 0.0611 (0.0643)  loss_box_reg: 0.0523 (0.0532)  loss_objectness: 0.0268 (0.0306)  loss_rpn_box_reg: 0.0193 (0.0166)  time: 0.1469  data: 0.0044  max mem: 1885\n",
            "Epoch: [9]  [160/200]  eta: 0:00:05  lr: 0.000000  loss: 0.1673 (0.1663)  loss_classifier: 0.0682 (0.0652)  loss_box_reg: 0.0528 (0.0542)  loss_objectness: 0.0294 (0.0306)  loss_rpn_box_reg: 0.0120 (0.0163)  time: 0.1461  data: 0.0045  max mem: 1885\n",
            "Epoch: [9]  [170/200]  eta: 0:00:04  lr: 0.000000  loss: 0.1647 (0.1658)  loss_classifier: 0.0610 (0.0649)  loss_box_reg: 0.0468 (0.0537)  loss_objectness: 0.0333 (0.0313)  loss_rpn_box_reg: 0.0096 (0.0159)  time: 0.1463  data: 0.0046  max mem: 1885\n",
            "Epoch: [9]  [180/200]  eta: 0:00:02  lr: 0.000000  loss: 0.1534 (0.1655)  loss_classifier: 0.0620 (0.0648)  loss_box_reg: 0.0396 (0.0535)  loss_objectness: 0.0303 (0.0314)  loss_rpn_box_reg: 0.0096 (0.0158)  time: 0.1464  data: 0.0045  max mem: 1885\n",
            "Epoch: [9]  [190/200]  eta: 0:00:01  lr: 0.000000  loss: 0.1692 (0.1669)  loss_classifier: 0.0650 (0.0654)  loss_box_reg: 0.0527 (0.0543)  loss_objectness: 0.0278 (0.0313)  loss_rpn_box_reg: 0.0106 (0.0159)  time: 0.1464  data: 0.0047  max mem: 1885\n",
            "Epoch: [9]  [199/200]  eta: 0:00:00  lr: 0.000000  loss: 0.1635 (0.1663)  loss_classifier: 0.0558 (0.0651)  loss_box_reg: 0.0570 (0.0544)  loss_objectness: 0.0240 (0.0309)  loss_rpn_box_reg: 0.0090 (0.0160)  time: 0.1457  data: 0.0046  max mem: 1885\n",
            "Epoch: [9] Total time: 0:00:29 (0.1481 s / it)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJHek0CfsBRZ"
      },
      "source": [
        "#Model Checkpoint save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQx3o948tKV2",
        "outputId": "4c41ec57-7da9-485d-b656-31234b3a2109"
      },
      "source": [
        "print(\"Our model: \\n\\n\", model, '\\n')\n",
        "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our model: \n",
            "\n",
            " FasterRCNN(\n",
            "  (transform): GeneralizedRCNNTransform(\n",
            "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
            "  )\n",
            "  (backbone): BackboneWithFPN(\n",
            "    (body): IntermediateLayerGetter(\n",
            "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "      (bn1): FrozenBatchNorm2d(64)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (layer1): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(64)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(64)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(256)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): FrozenBatchNorm2d(256)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(64)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(64)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(256)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(64)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(64)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(256)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer2): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(128)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(128)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(512)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d(512)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(128)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(128)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(512)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(128)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(128)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(512)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(128)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(128)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(512)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer3): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d(1024)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer4): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(512)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(512)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(2048)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d(2048)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(512)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(512)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(2048)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(512)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(512)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(2048)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (fpn): FeaturePyramidNetwork(\n",
            "      (inner_blocks): ModuleList(\n",
            "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (layer_blocks): ModuleList(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (extra_blocks): LastLevelMaxPool()\n",
            "    )\n",
            "  )\n",
            "  (rpn): RegionProposalNetwork(\n",
            "    (anchor_generator): AnchorGenerator()\n",
            "    (head): RPNHead(\n",
            "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): RoIHeads(\n",
            "    (box_roi_pool): MultiScaleRoIAlign()\n",
            "    (box_head): TwoMLPHead(\n",
            "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (box_predictor): FastRCNNPredictor(\n",
            "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=20, bias=True)\n",
            "    )\n",
            "  )\n",
            ") \n",
            "\n",
            "The state dict keys: \n",
            "\n",
            " odict_keys(['backbone.body.conv1.weight', 'backbone.body.bn1.weight', 'backbone.body.bn1.bias', 'backbone.body.bn1.running_mean', 'backbone.body.bn1.running_var', 'backbone.body.layer1.0.conv1.weight', 'backbone.body.layer1.0.bn1.weight', 'backbone.body.layer1.0.bn1.bias', 'backbone.body.layer1.0.bn1.running_mean', 'backbone.body.layer1.0.bn1.running_var', 'backbone.body.layer1.0.conv2.weight', 'backbone.body.layer1.0.bn2.weight', 'backbone.body.layer1.0.bn2.bias', 'backbone.body.layer1.0.bn2.running_mean', 'backbone.body.layer1.0.bn2.running_var', 'backbone.body.layer1.0.conv3.weight', 'backbone.body.layer1.0.bn3.weight', 'backbone.body.layer1.0.bn3.bias', 'backbone.body.layer1.0.bn3.running_mean', 'backbone.body.layer1.0.bn3.running_var', 'backbone.body.layer1.0.downsample.0.weight', 'backbone.body.layer1.0.downsample.1.weight', 'backbone.body.layer1.0.downsample.1.bias', 'backbone.body.layer1.0.downsample.1.running_mean', 'backbone.body.layer1.0.downsample.1.running_var', 'backbone.body.layer1.1.conv1.weight', 'backbone.body.layer1.1.bn1.weight', 'backbone.body.layer1.1.bn1.bias', 'backbone.body.layer1.1.bn1.running_mean', 'backbone.body.layer1.1.bn1.running_var', 'backbone.body.layer1.1.conv2.weight', 'backbone.body.layer1.1.bn2.weight', 'backbone.body.layer1.1.bn2.bias', 'backbone.body.layer1.1.bn2.running_mean', 'backbone.body.layer1.1.bn2.running_var', 'backbone.body.layer1.1.conv3.weight', 'backbone.body.layer1.1.bn3.weight', 'backbone.body.layer1.1.bn3.bias', 'backbone.body.layer1.1.bn3.running_mean', 'backbone.body.layer1.1.bn3.running_var', 'backbone.body.layer1.2.conv1.weight', 'backbone.body.layer1.2.bn1.weight', 'backbone.body.layer1.2.bn1.bias', 'backbone.body.layer1.2.bn1.running_mean', 'backbone.body.layer1.2.bn1.running_var', 'backbone.body.layer1.2.conv2.weight', 'backbone.body.layer1.2.bn2.weight', 'backbone.body.layer1.2.bn2.bias', 'backbone.body.layer1.2.bn2.running_mean', 'backbone.body.layer1.2.bn2.running_var', 'backbone.body.layer1.2.conv3.weight', 'backbone.body.layer1.2.bn3.weight', 'backbone.body.layer1.2.bn3.bias', 'backbone.body.layer1.2.bn3.running_mean', 'backbone.body.layer1.2.bn3.running_var', 'backbone.body.layer2.0.conv1.weight', 'backbone.body.layer2.0.bn1.weight', 'backbone.body.layer2.0.bn1.bias', 'backbone.body.layer2.0.bn1.running_mean', 'backbone.body.layer2.0.bn1.running_var', 'backbone.body.layer2.0.conv2.weight', 'backbone.body.layer2.0.bn2.weight', 'backbone.body.layer2.0.bn2.bias', 'backbone.body.layer2.0.bn2.running_mean', 'backbone.body.layer2.0.bn2.running_var', 'backbone.body.layer2.0.conv3.weight', 'backbone.body.layer2.0.bn3.weight', 'backbone.body.layer2.0.bn3.bias', 'backbone.body.layer2.0.bn3.running_mean', 'backbone.body.layer2.0.bn3.running_var', 'backbone.body.layer2.0.downsample.0.weight', 'backbone.body.layer2.0.downsample.1.weight', 'backbone.body.layer2.0.downsample.1.bias', 'backbone.body.layer2.0.downsample.1.running_mean', 'backbone.body.layer2.0.downsample.1.running_var', 'backbone.body.layer2.1.conv1.weight', 'backbone.body.layer2.1.bn1.weight', 'backbone.body.layer2.1.bn1.bias', 'backbone.body.layer2.1.bn1.running_mean', 'backbone.body.layer2.1.bn1.running_var', 'backbone.body.layer2.1.conv2.weight', 'backbone.body.layer2.1.bn2.weight', 'backbone.body.layer2.1.bn2.bias', 'backbone.body.layer2.1.bn2.running_mean', 'backbone.body.layer2.1.bn2.running_var', 'backbone.body.layer2.1.conv3.weight', 'backbone.body.layer2.1.bn3.weight', 'backbone.body.layer2.1.bn3.bias', 'backbone.body.layer2.1.bn3.running_mean', 'backbone.body.layer2.1.bn3.running_var', 'backbone.body.layer2.2.conv1.weight', 'backbone.body.layer2.2.bn1.weight', 'backbone.body.layer2.2.bn1.bias', 'backbone.body.layer2.2.bn1.running_mean', 'backbone.body.layer2.2.bn1.running_var', 'backbone.body.layer2.2.conv2.weight', 'backbone.body.layer2.2.bn2.weight', 'backbone.body.layer2.2.bn2.bias', 'backbone.body.layer2.2.bn2.running_mean', 'backbone.body.layer2.2.bn2.running_var', 'backbone.body.layer2.2.conv3.weight', 'backbone.body.layer2.2.bn3.weight', 'backbone.body.layer2.2.bn3.bias', 'backbone.body.layer2.2.bn3.running_mean', 'backbone.body.layer2.2.bn3.running_var', 'backbone.body.layer2.3.conv1.weight', 'backbone.body.layer2.3.bn1.weight', 'backbone.body.layer2.3.bn1.bias', 'backbone.body.layer2.3.bn1.running_mean', 'backbone.body.layer2.3.bn1.running_var', 'backbone.body.layer2.3.conv2.weight', 'backbone.body.layer2.3.bn2.weight', 'backbone.body.layer2.3.bn2.bias', 'backbone.body.layer2.3.bn2.running_mean', 'backbone.body.layer2.3.bn2.running_var', 'backbone.body.layer2.3.conv3.weight', 'backbone.body.layer2.3.bn3.weight', 'backbone.body.layer2.3.bn3.bias', 'backbone.body.layer2.3.bn3.running_mean', 'backbone.body.layer2.3.bn3.running_var', 'backbone.body.layer3.0.conv1.weight', 'backbone.body.layer3.0.bn1.weight', 'backbone.body.layer3.0.bn1.bias', 'backbone.body.layer3.0.bn1.running_mean', 'backbone.body.layer3.0.bn1.running_var', 'backbone.body.layer3.0.conv2.weight', 'backbone.body.layer3.0.bn2.weight', 'backbone.body.layer3.0.bn2.bias', 'backbone.body.layer3.0.bn2.running_mean', 'backbone.body.layer3.0.bn2.running_var', 'backbone.body.layer3.0.conv3.weight', 'backbone.body.layer3.0.bn3.weight', 'backbone.body.layer3.0.bn3.bias', 'backbone.body.layer3.0.bn3.running_mean', 'backbone.body.layer3.0.bn3.running_var', 'backbone.body.layer3.0.downsample.0.weight', 'backbone.body.layer3.0.downsample.1.weight', 'backbone.body.layer3.0.downsample.1.bias', 'backbone.body.layer3.0.downsample.1.running_mean', 'backbone.body.layer3.0.downsample.1.running_var', 'backbone.body.layer3.1.conv1.weight', 'backbone.body.layer3.1.bn1.weight', 'backbone.body.layer3.1.bn1.bias', 'backbone.body.layer3.1.bn1.running_mean', 'backbone.body.layer3.1.bn1.running_var', 'backbone.body.layer3.1.conv2.weight', 'backbone.body.layer3.1.bn2.weight', 'backbone.body.layer3.1.bn2.bias', 'backbone.body.layer3.1.bn2.running_mean', 'backbone.body.layer3.1.bn2.running_var', 'backbone.body.layer3.1.conv3.weight', 'backbone.body.layer3.1.bn3.weight', 'backbone.body.layer3.1.bn3.bias', 'backbone.body.layer3.1.bn3.running_mean', 'backbone.body.layer3.1.bn3.running_var', 'backbone.body.layer3.2.conv1.weight', 'backbone.body.layer3.2.bn1.weight', 'backbone.body.layer3.2.bn1.bias', 'backbone.body.layer3.2.bn1.running_mean', 'backbone.body.layer3.2.bn1.running_var', 'backbone.body.layer3.2.conv2.weight', 'backbone.body.layer3.2.bn2.weight', 'backbone.body.layer3.2.bn2.bias', 'backbone.body.layer3.2.bn2.running_mean', 'backbone.body.layer3.2.bn2.running_var', 'backbone.body.layer3.2.conv3.weight', 'backbone.body.layer3.2.bn3.weight', 'backbone.body.layer3.2.bn3.bias', 'backbone.body.layer3.2.bn3.running_mean', 'backbone.body.layer3.2.bn3.running_var', 'backbone.body.layer3.3.conv1.weight', 'backbone.body.layer3.3.bn1.weight', 'backbone.body.layer3.3.bn1.bias', 'backbone.body.layer3.3.bn1.running_mean', 'backbone.body.layer3.3.bn1.running_var', 'backbone.body.layer3.3.conv2.weight', 'backbone.body.layer3.3.bn2.weight', 'backbone.body.layer3.3.bn2.bias', 'backbone.body.layer3.3.bn2.running_mean', 'backbone.body.layer3.3.bn2.running_var', 'backbone.body.layer3.3.conv3.weight', 'backbone.body.layer3.3.bn3.weight', 'backbone.body.layer3.3.bn3.bias', 'backbone.body.layer3.3.bn3.running_mean', 'backbone.body.layer3.3.bn3.running_var', 'backbone.body.layer3.4.conv1.weight', 'backbone.body.layer3.4.bn1.weight', 'backbone.body.layer3.4.bn1.bias', 'backbone.body.layer3.4.bn1.running_mean', 'backbone.body.layer3.4.bn1.running_var', 'backbone.body.layer3.4.conv2.weight', 'backbone.body.layer3.4.bn2.weight', 'backbone.body.layer3.4.bn2.bias', 'backbone.body.layer3.4.bn2.running_mean', 'backbone.body.layer3.4.bn2.running_var', 'backbone.body.layer3.4.conv3.weight', 'backbone.body.layer3.4.bn3.weight', 'backbone.body.layer3.4.bn3.bias', 'backbone.body.layer3.4.bn3.running_mean', 'backbone.body.layer3.4.bn3.running_var', 'backbone.body.layer3.5.conv1.weight', 'backbone.body.layer3.5.bn1.weight', 'backbone.body.layer3.5.bn1.bias', 'backbone.body.layer3.5.bn1.running_mean', 'backbone.body.layer3.5.bn1.running_var', 'backbone.body.layer3.5.conv2.weight', 'backbone.body.layer3.5.bn2.weight', 'backbone.body.layer3.5.bn2.bias', 'backbone.body.layer3.5.bn2.running_mean', 'backbone.body.layer3.5.bn2.running_var', 'backbone.body.layer3.5.conv3.weight', 'backbone.body.layer3.5.bn3.weight', 'backbone.body.layer3.5.bn3.bias', 'backbone.body.layer3.5.bn3.running_mean', 'backbone.body.layer3.5.bn3.running_var', 'backbone.body.layer4.0.conv1.weight', 'backbone.body.layer4.0.bn1.weight', 'backbone.body.layer4.0.bn1.bias', 'backbone.body.layer4.0.bn1.running_mean', 'backbone.body.layer4.0.bn1.running_var', 'backbone.body.layer4.0.conv2.weight', 'backbone.body.layer4.0.bn2.weight', 'backbone.body.layer4.0.bn2.bias', 'backbone.body.layer4.0.bn2.running_mean', 'backbone.body.layer4.0.bn2.running_var', 'backbone.body.layer4.0.conv3.weight', 'backbone.body.layer4.0.bn3.weight', 'backbone.body.layer4.0.bn3.bias', 'backbone.body.layer4.0.bn3.running_mean', 'backbone.body.layer4.0.bn3.running_var', 'backbone.body.layer4.0.downsample.0.weight', 'backbone.body.layer4.0.downsample.1.weight', 'backbone.body.layer4.0.downsample.1.bias', 'backbone.body.layer4.0.downsample.1.running_mean', 'backbone.body.layer4.0.downsample.1.running_var', 'backbone.body.layer4.1.conv1.weight', 'backbone.body.layer4.1.bn1.weight', 'backbone.body.layer4.1.bn1.bias', 'backbone.body.layer4.1.bn1.running_mean', 'backbone.body.layer4.1.bn1.running_var', 'backbone.body.layer4.1.conv2.weight', 'backbone.body.layer4.1.bn2.weight', 'backbone.body.layer4.1.bn2.bias', 'backbone.body.layer4.1.bn2.running_mean', 'backbone.body.layer4.1.bn2.running_var', 'backbone.body.layer4.1.conv3.weight', 'backbone.body.layer4.1.bn3.weight', 'backbone.body.layer4.1.bn3.bias', 'backbone.body.layer4.1.bn3.running_mean', 'backbone.body.layer4.1.bn3.running_var', 'backbone.body.layer4.2.conv1.weight', 'backbone.body.layer4.2.bn1.weight', 'backbone.body.layer4.2.bn1.bias', 'backbone.body.layer4.2.bn1.running_mean', 'backbone.body.layer4.2.bn1.running_var', 'backbone.body.layer4.2.conv2.weight', 'backbone.body.layer4.2.bn2.weight', 'backbone.body.layer4.2.bn2.bias', 'backbone.body.layer4.2.bn2.running_mean', 'backbone.body.layer4.2.bn2.running_var', 'backbone.body.layer4.2.conv3.weight', 'backbone.body.layer4.2.bn3.weight', 'backbone.body.layer4.2.bn3.bias', 'backbone.body.layer4.2.bn3.running_mean', 'backbone.body.layer4.2.bn3.running_var', 'backbone.fpn.inner_blocks.0.weight', 'backbone.fpn.inner_blocks.0.bias', 'backbone.fpn.inner_blocks.1.weight', 'backbone.fpn.inner_blocks.1.bias', 'backbone.fpn.inner_blocks.2.weight', 'backbone.fpn.inner_blocks.2.bias', 'backbone.fpn.inner_blocks.3.weight', 'backbone.fpn.inner_blocks.3.bias', 'backbone.fpn.layer_blocks.0.weight', 'backbone.fpn.layer_blocks.0.bias', 'backbone.fpn.layer_blocks.1.weight', 'backbone.fpn.layer_blocks.1.bias', 'backbone.fpn.layer_blocks.2.weight', 'backbone.fpn.layer_blocks.2.bias', 'backbone.fpn.layer_blocks.3.weight', 'backbone.fpn.layer_blocks.3.bias', 'rpn.head.conv.weight', 'rpn.head.conv.bias', 'rpn.head.cls_logits.weight', 'rpn.head.cls_logits.bias', 'rpn.head.bbox_pred.weight', 'rpn.head.bbox_pred.bias', 'roi_heads.box_head.fc6.weight', 'roi_heads.box_head.fc6.bias', 'roi_heads.box_head.fc7.weight', 'roi_heads.box_head.fc7.bias', 'roi_heads.box_predictor.cls_score.weight', 'roi_heads.box_predictor.cls_score.bias', 'roi_heads.box_predictor.bbox_pred.weight', 'roi_heads.box_predictor.bbox_pred.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "DHfgdaZmtR7R",
        "outputId": "26f8f9a4-b543-438a-b0ec-512cf0115cb4"
      },
      "source": [
        "from google.colab import files\n",
        "torch.save(model.state_dict(), 'checkpoint.pth')\n",
        "\n",
        "# download checkpoint file\n",
        "files.download('checkpoint.pth')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_f57325da-3395-4ee6-97b8-89c176f7a2a8\", \"checkpoint.pth\", 165791006)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6mYGFLxkO8F"
      },
      "source": [
        "Now that training has finished, let's have a look at what it actually predicts in a test image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHwIdxH76uPj"
      },
      "source": [
        "# pick one image from the test set\n",
        "img, _ = dataset_test[0]\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmN602iKsuey"
      },
      "source": [
        "Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.\n",
        "The dictionary contains the predictions for the image we passed. In this case, we can see that it contains `boxes`, `labels`, `masks` and `scores` as fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkmb3qUu6zw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2217c450-6f6a-4a3d-812a-299765139e98"
      },
      "source": [
        "prediction"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'boxes': tensor([[  0.0000,  88.0800,  27.6211, 106.8034],\n",
              "          [  0.0000,   8.2850, 151.5132, 162.8147],\n",
              "          [  0.0000,  75.7268, 224.0000, 156.6401],\n",
              "          [ 17.9723,  60.7403, 185.7222, 138.0727],\n",
              "          [  0.0000, 170.9290, 124.7929, 200.4859],\n",
              "          [ 22.5377,  27.6003, 220.2709,  63.3934],\n",
              "          [ 16.7467,  38.7473, 128.0023,  99.6804],\n",
              "          [185.1766, 185.2156, 211.4926, 210.5097],\n",
              "          [166.7910, 150.3871, 208.7434, 207.0043],\n",
              "          [ 21.9000,  37.0759, 218.2239, 195.6512],\n",
              "          [ 42.7215,  64.1217, 205.9946,  96.2052],\n",
              "          [ 10.4787,  57.7156, 175.5271,  87.4769],\n",
              "          [  0.0000, 157.1068, 146.3099, 186.9339],\n",
              "          [  0.0000,  10.3850, 148.4418, 172.2945],\n",
              "          [134.8318,  69.4643, 204.8895, 136.3717],\n",
              "          [ 20.7710,  45.5430, 197.1770,  79.7840],\n",
              "          [  0.0000,  41.0303, 224.0000,  96.2940],\n",
              "          [  9.0015,  66.4285, 132.7011,  95.5708],\n",
              "          [ 98.3184, 143.5949, 206.1450, 209.1509],\n",
              "          [  0.0000, 174.9047, 224.0000, 201.7202],\n",
              "          [ 28.1691,  72.4766, 176.0929, 105.6953],\n",
              "          [  0.0000, 147.9722, 135.8662, 176.5365],\n",
              "          [178.4885, 158.1706, 218.0835, 188.5982],\n",
              "          [ 11.4665,  36.8627, 121.6272,  69.3348],\n",
              "          [ 82.8445,  30.4623, 219.8576,  80.1843],\n",
              "          [157.6230,  80.3659, 208.3847,  99.1860],\n",
              "          [ 76.9650, 161.4381, 224.0000, 195.0516],\n",
              "          [138.7350, 152.3854, 216.0198, 168.7136],\n",
              "          [177.8238,  62.5955, 221.0230,  91.9051],\n",
              "          [178.5282,  49.6646, 218.2777,  80.9478],\n",
              "          [ 61.5111,  41.3490, 168.7013,  92.5003],\n",
              "          [  0.0000, 145.7932, 223.4282, 201.4359],\n",
              "          [ 82.3082, 143.7294, 223.8451, 176.4575],\n",
              "          [ 20.6886, 162.5477, 181.8868, 196.2190],\n",
              "          [ 68.3431,  22.3075, 217.0983,  53.4296],\n",
              "          [ 47.6684,  21.1726, 149.3069,  82.0487],\n",
              "          [116.2082, 152.8118, 215.7107, 215.4646],\n",
              "          [ 10.4896,  43.6776, 110.7943,  78.9967],\n",
              "          [  6.8038, 146.4931,  21.8417, 174.7808],\n",
              "          [ 63.2399, 146.9206, 166.8575, 208.7138],\n",
              "          [  9.6007,  63.9922, 119.6778,  97.4667],\n",
              "          [ 16.9205,  54.4353, 163.7127,  89.4599],\n",
              "          [  0.0000,  62.8774, 224.0000, 151.3847],\n",
              "          [  8.9614,  25.7675, 106.5972,  63.1163],\n",
              "          [ 18.6345, 140.4334, 130.4063, 193.3761],\n",
              "          [ 86.3055, 177.0867, 224.0000, 216.7740]], device='cuda:0'),\n",
              "  'labels': tensor([3, 3, 1, 3, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1,\n",
              "          1, 3, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 3, 3, 3, 1, 3, 3, 2, 3, 1, 3],\n",
              "         device='cuda:0'),\n",
              "  'scores': tensor([0.3395, 0.2070, 0.1598, 0.1403, 0.1376, 0.1155, 0.1136, 0.1130, 0.1121,\n",
              "          0.1121, 0.1041, 0.0998, 0.0970, 0.0969, 0.0920, 0.0905, 0.0894, 0.0871,\n",
              "          0.0864, 0.0858, 0.0858, 0.0811, 0.0797, 0.0790, 0.0784, 0.0782, 0.0772,\n",
              "          0.0766, 0.0727, 0.0702, 0.0701, 0.0678, 0.0655, 0.0614, 0.0595, 0.0589,\n",
              "          0.0568, 0.0564, 0.0559, 0.0547, 0.0537, 0.0535, 0.0518, 0.0510, 0.0506,\n",
              "          0.0504], device='cuda:0')}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwT21rzotFbH"
      },
      "source": [
        "Let's inspect the image and the predicted segmentation masks.\n",
        "\n",
        "For that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in `[C, H, W]` format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpqN9t1u7B2J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "dbabaa66-195e-46a7-fbbf-2d681213c0e5"
      },
      "source": [
        "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAg3UlEQVR4nO3dd5wddb3/8df7JOCNImWzJYUSCVzUi/pTVNTrTxQVC0oJJQLi/XF/eqUIUWmCXn/2RlQgESmBkJ6QQOqm7W56gIQQuhdIEAgttN1sTT37+f0x+R5mT8vs7tmdgf3+s/N8nN05c87Mme9rZ06TwIQAiwuTxAnYHtPxYi7WZjonGpZhLabTxUqs0XSqWIU1mE4RK7EW08liOdZsOkXUuD/OwgixFGsxjRK3Yy2m9eJLWNr04XhWiyCNpdBt0ICNQT/vebzQi3fwYjgBW4GOhBSWRioIFMwpD48c/AkBQ9E4dCMcjsah6zqDJ+FwdCoCDi6KAQjol30zRPEHu4dHrJDiT/w14lxst+krrrYjIuBMscLlewXW5Mre6NBk+qZYhTWbTo6Q+EvFeKzFtFqcjO0yfTSe1XIUrMCGo7Hv0MT/CsqwejTAJ96jOwgnvq1LiScyfoGAwT7xHm8rJCHxZ4krsVbTuWJuBNRizabTRF0OlmONOQj+DQjmOj0HI0QN1my6UEzBmk014gys1fTJeFZLbx68x5L46/Zddp94jwgIJ35GlxKfwa2R0fFmiOIPdg+PWBFj4reIMsxMnxO3YK2mc8S8jhgpFnTE2WKxO4qvwxpNI0Qd1mI6NV/0C+FMUdvxcP57YhrWYlogvhVD4g+ERqw/uvkdmvhD4UHsODTKJ96jJPgTetQlvmtl7wLej9pggE+8x9sBSUj80Tllz+BsUZ2DvInPIOh4jTtCX4o1m87IwdliUUdcIiZizaZq8S1sl+n4Xl0th8IL2P7opjgSfwJMws5HdA8fgquw4OgqF8OxZ1DKJ96jJLgFPdpbZc/gP1AbDNx7M/pb8BgNdmKPdww+he7HPoXoBsaj8WgLNg4dj7ZgjyFKhE+iLdgANKAjTkKz0ZvYFFzipyDgvAI4Ho7CNiNKhHfBYdgLW5QqY6jZS0eLeQxttZfOycHZojoHixnaYi+dKeoY2mgvjdCAOsoarX6EqKOs2epPEzWUNVv9CLGEsmarP1NUU9Zi9SPFfMrarP5cMacjRonbKGuz+nka8C0G7LLtnxMwANtOdzEAwEAUxiswGLsPPRFH4rdEuIXdxFiX750+8R7R8QUEfCEfxsWR+AmoDc5AE4IH99/RCgSs6Dbc7uLhUTKopwf2glgthoMZx4l50Gqck4OzRXUOFkOLcaaog0ZjREc0G6eJGmg2Rogl0GycKaqhxRgp5kOrcZ6Y0xGXiduh1bhHnA+txhd7dbXUQxn2JFrTi2WfBG1YLfpzz9/B0Z0oeyjxYxAwpijWIWBdt/EEAp5IwH7pEQWxJP5h1AZj3roZca8FD48iUGyJHyc+De3GlxOW+Oni/8aQ+AzG9XzZfw0N2C1oAxjWggb12N3Z43B91xIf+y7ikVhMQY/Ccb2b+O2oDd5462bEvRY8PIpAsSX+J+JcaDJGiMXQbJyVgzNDjc5FrZs9jJZoiT9XzC2Q+Gniu3Em/iHYga1DB5S67H+ERuxOVAPtWBM6rOfvTvcSr3i2gcfbAi/CbmwL2tTz/xaPgSbsLjS/w82AGEdvD499QrHtoN8Uvw2lucE4qzCCoG/LQZHENxlndCnxk8SFvZr4ndi7QtgFhu1Ed5V6lNrsrvnAXtzcPvEePY750IzNRCeXep/5m7vm2fmXDjGO3h4e+4R6e3dsD15GY3xMTMjX6Lw4XdRCcwQ0GmeIGmjseMgfPfETxUXQanxepADr6c4sxU5CwGLsq2gVtGF16JhSD1ePQxrbht7Xa5vbJ96jF9AOYIZegt3Yc2go7MSeQB+LgP8Ne7Dn0HGQxhrRIDBsBzpkH0uH3h+0PTyiQ729O74hDgAzjhKLeiDx24wzQ4nPwtliSUeMFIuh0bhYTIFG43bxY2g0Ptsbid8M/8RaURs0Yreji3rsXGPmRP39vba5Q4mf4aq91Sfe450CKO2A7OFRUvT+UjeJNtECVeJhsRoqu4E1UCEeCWEFVIiHC6AqB0McPuLwHYfdol2098jauBuNh5N694VCGTyLnoLqmB52aZSGNGpDbdCGXkGvwCv5EMyTkPHcwyML0Mt7j4dHZ9D7S60TjaIeqsQGsQIqxQNieTRUiYfEatfoEib+2JzE7xB7xJ5SroRZ6A44Maay5yLux9++EUwSMp57eGQBYt9FPDwKo/eXeovYKrZAlbhXLIXKaLhP1IT+McgkvlI8KtY6rCiKQeIxV/YsHOvwHYceSHxCyp5BNZoBI+N/IBZEMEnIeO7hkQWIfRfx8CiM3l/q1eJZ8VTHxK+NjCrxgKiDKrFR1EGleEQscx1fDpVio6jNh6ocDBEPiTo41uE8hyaxS+wq5UpISNl94j08SgKIfRfx8CiM3l/q6WKjWAtVnSl7jyY+wL85fMshSPzOUq6EhJTdJ97DoySA2HcRD4/C6LWFpYUJg4+GYt01ZCV+kEt8lXjQnczvAo5xOMvhddEmWku5NhJSdp94D4+SAGLfRTw8CqPXFrZV7BQ74H1ilVgEVRFQGcIah3Xu4H29w0Z3FL/BHfJ3Acc4nOXgE58ABJOEjOceHlmA2HcRD4/C6LWFPSIaRQOUi1ViXmdQDeVijagunPi6yEFfLxbnwzEOZzq8+s5P/BNoI9wZ/wOxIIJJQsZzD48sQOy7iIdHYfTawuaJV8VLoXwPjIxw4ivE/aIWKkKJfyCEpQWQSfw6V/YsHONwusMmsU00lHJtzEWT4ZS4y+7fNOfhURJA7LuIh0dh9NrCbhBbxKZSJH6tWAIVoRfkZ47r14klHbHe4X4X9HsL4BiH0x16IPEZTEQ3wrG9G/Rgof8W/8OuEwgmCRnPPTyyALHvIh4ehdFrC/uxeEY81o3Er3aJXy0Wh1Dpoh/8G7A4Gipz8K8OpzisF6+Il3t2/WxG/4A5aDaaAF9Dk9AY+HC3g347Gg3D0GI0E76NdqDt0BD/w64TCCYJGc89PLIAse8iHh6F0WsLO1tsEg8nOPHDHb7ucK94SbwY2+YxZNCOmlADPIdeQM/C8miI+7FVGgSThIznHh5ZgNh3EQ+PwujpZaQFwuDfxZNiIwxyb38L8r0wGnLL3nOJ/6pDndginkvCduq7CCYJGc89PLIAse8iHh6F0dPLeEnsFjvhQ+IJsR4OFevEMqgQK8XcyAgfxYdR6Vpfng/hsi8KoTIHwx2+6jBbbBZPJ2E79V0Ek4SM5x4eWYDYdxEPj8Lo6WU8KFpFExzpPlL+0NBL4leIOVAhlovZUN5JrBQLoMId6ZfnQ3cSP0E8IR5PwnbquwgmCRnPPTyyALHvIh4ehdHTy5gt6sVrMEisF8vdifolpUt8+L3zWcicww+Qt+wZDHP4ssNYsVFsSMJ26rsIJgkZzz08sgCx7yIeHoXR08v4q3hJPOu+dGaJ++S66k4mfmA+BLMXQd6gry6AYQ5fdrhePCDWJ2E79V0Ek4SM5x4eWYDYdxEPj8Lo6WWMci+kD8o+H6rESjHPJT7T8XuKYmAIy8Td0RAczs9xmOs+EXdBPgxzONHhOnG/uD8J26nvIpgkZDz38MgCxL6LeHgURk8v4zTxiPteuZViTgiVocQvFTOgXCwS0/KhLB9qxF35MFDUiplQLuo6olLUiVn5MMzh86JOzIXfi3vFfUnYTn0XwSQh47mHRxYg9l3Ew6MweuiqM98r97/Eve6bZZaJexzuhipRK2bBQLFATIaBYraYEEJZUQRzlYlqMSWEUiX+N2KNT3zMCCYJGc89PLIAse8iHh5FoB55+D8vKqHdOEZMhG3GRWIaNBgXigmwzRglboNG48diDGw3rsrBlWJsDm6CNuMKMQEajctyMErMgAbjIjED6kO4xN2MS8TUjrhSjIMGY6z4b2gxNoovQX/j06VeP28HpB2eyfmVgTDQk+6SdgCTmyuFHgVhKbTHIfhVP/Q8pLAUqneXpAsjKavDw8Mn3uPtB/XIw3+N+CDsMT7sAnpxKPF3QINxubgVGo3LxY3QalyTg5+IMTnoocSPEhOhwRgtfgvbjPvF1wHjC2/dwf+BFBZ87lwWHnXYBSkMtAvkEFxyH6Sw4/NhLAzHvob+DB/ExqHPhJCpZBZ25mBPCClMqLXw7G2FfxUd/dFuELafQ//SodQPTQ+PUgK6MOp6ePQaVNpH/d5Dvyl6+QTYZXxGr02GbcYPtG4y2ma6SNvvQA2my7X0VtRoqcu1+0bUarpG7Q6rbkStlvqJ0mNycBNqs9QVSk9AjZa6TOkJ0Gipy9Qe4MdqnwoNlrpE7dOg3lKXKD0N1VvqB0pPJdVguiQHo5SeSKrB9Eel/0Bqm2mt0t8khekLaocuFLBrc3XqmgMoByVcxD7vjk+8R5+FSMbt6Oswhz0O6RzsdNgB6jiXhX4FJndJcbS7ud5wl4ShjpfUuksaIWscXZdzSRZyB/giIHSe1ZDem31RJGQdwOKWkQ6OHH+n9Jmktpu+ovRkUttMP1B6Iqkm00VK30GqwXS50reSajRdvrfsXKP2G0mF8RO1j8nBTaTaTFcoPYFUo+myDq1Xo6WuUHoaarDUxUpPR/WWutgl/hKlp5GqN+XiMqUnk6o3/UHpP5GqN61W+lSfeJ94D4+CSFLiGxwywaqPMFeRv4kyewafd5jqkDmL2ezQwN5Rap27pN5dkhnAnnBnH3cXPmOaN0HR/9jcWdX2HFjO7dknwmdM0xEQnivieBw56LkobeJNaZHC9H2lv0+q1XRWhMRfqfbrUavxU7VfTyqMa9V+Qw7GkmozXaX0eFKNph8qPR41WiqMqajBUpfk4AdKTy+Q+B8qPbFj4lcqfTopM33JJ94n3uPtgDfgAMxQawjvwQy9DAdhhrY6NLi/aYZ/wQxthv6uTlnX/FL+hYoxaCfshwm9CJWYodcdXoNyzNAGeB/Wjp6BIVg7ehkOwQw9AO/F2lFTzjJeADC5o8vuryCFIDd4l+SaN7mbOom9ofh94ZJGR2cTH/2aO1VkFR50+1Li00qnSGH6htLXkNpuOj9y4luMn6n9elJh/FTtN+RDq+lapW8l1WS6QulbUZOlwrgTNVjqR/kwhVS96dKcE/U/UvpOUg2m3yo9mlSDqVbps0ilTV/zifeJfxviZTgYM9QYQlC3La48m6Eca0evwUFYO9oG78YMPQLvwkAtbvA2hzfdIt7MWWhmmHgTgsGlGQ5w1/NuDPQslGGg1+FAV9sBmFCTi+zz7nracxbxYjJWLwBCibgdUZFyUKmveTYED46H3JbziX/HJX670vuTwvRxpa8jtd30vXyJH0+qyTQqJ/FXq/16Um3GtfuGfqb0WFLNpmuUHouaLXWN0sGT+1cpPc6dsb8tBxPdM+9Zib9M6UmkGky/UPoGUg2mhUqfR2qX6etqF8JS6tOJN4cKwCFYaGUIhJBCB7pLop/5EtrfzdWDo92LcBAmtNUdTt0HR2Bp9DxUYYZecU3c4A4GXwk9+oPraQhdYaFlNcMAzNAOh+0OW+FAdwz3bnfgGbR1h+vdVuiH4Z5pVGjpnTqZ6lFq+MQ7/IW9fdjm4BP/jkv8ZqUHk2o3fdB1/OJOJv7PpFqM/7dv6JdKjybVZvq50n8m1Wb6b6VvJtVsukrp2/Zec2YRb2ECqW2mUUpPJtVgutQd1/9Q6Umk3jT9yiV+jtIXkNplOknt/RCW6hcpjpnVMsThUFAIoKEdITTYXdIPwqO4im6LzI76HocDcpD7q3cnY7iJAg11Sju05lyS+evtybjRHn0GPvEOPvF9IPErVHUspI2PavgEaDIu0ZGT0DazSzV8ImwzXaIjx0OTMUqbgjfNXSmuhxbjajEaWoxfiNHQbPzyLQwbjZqNX+qI0ajZ7Nc65DpoMn4tMhgDTcbPxK1om9nV0i2wzbhaOOw3Ye+78vcL3rB/mfafAvXGD7XfJHjT+JX2vwEajDna/wLYZZwo7QcY+yVlR+07KP1Q5OFROvjEO5Q88QadaVn+xEfMbmcTn+qItDvLZm7pg9zsR7lbmMHRIYAphIoe2O7ucVoSTBTPic1QJZaKu/KhQlSLqVAhZos7oUJMEbfAwWKcuN7hr0VRJm7PwRTxdygTd4nboDyEWeJ2KHefgFcuFoqpUC6WiGnuFk6DD7gP1D1HLBULoEnsEjtLt6I8IsMn3iPJ8Il3yCT+Y4ArYBYGuL8Z5nBgMlbLOxjucVoS/EY8IR4qmvhKsVhMg0oxT0yECjFN3FrqxN+aD+VuoZ1KfKNPfGzwifdIMrqa+Kwn4vaJktzo8PWUZL9KhW7haAiqfWn8W8XjLbjHaXeQ+UT674sHxMqiiS8X88Ukd2Q9Dspd4g9xiT9E/E38KTJuFX/uduIHi1oxDY4VK8QsOEesENXwqmgTrd1eUR6dh0+8R4+g+40VgOgX+arNNTGMfqg9B/u7KzzEzX4QgKXQex0Ocn9zsMNH3B8Pdcic+K10OAeOw/qjlfDvEXAi1g8tg5OxFFoIp2FCs+E0rB+6G87D2tH7YRgG+kQCtq5HBgWH1k5gu0iLNHxTrBFLoNJ9H1wYmTPkM9zJ8wymuRD3dOIrQqcOFrrnC5aK6XCYWClmwofEajEbviVWi2rY6hMfG3zi35bIrVzmfG1n2xoFh7ilV7hLKjpeQuh9IOXub8ocFGERKYdMWg8BEItCke34uw79HZCArQIaDHOwZnR+NCzAtqOzYAG2E50B1dgOdAYswbajEbAI247uhS9jhj6UjHvqEaAU43C9O4/9cVHrvig2CGgYURJ/pxjTjcQXKnv4KP4eMb4j5ok7YbBYIibD+8VyMRPOEMvFPHhGNIpt3V5RHp2HT3wp0d9hsEPmDQtDIqCywDXnbWJ/h6z3bwx1HyMzNHR73pOD/gWWlTBA7LtIFGxFO6ANDUaPwQMF8Gg+bCiKoQ5XoMfhsbjvqUdHlOSKHhONoh6Ginnu0DhzsJzBXDGhQOKnuxBPFH/rRuJnuOsphHJxt7ijI+aKO9yLBCbC0e57508TNWIubBINoiEJG6zPYW/ic7+FrlMHerlzuQXsA5niHOEwzF3P4Q6Hdbwk8zd08hWy/Rwyp2n3i4DMH6szy/IoESD2XSQKVqEd0IoGoxfhWTQYbcnB8zn4JzyJBqNn4PF8GOrwPYeE3GWP4GcprmixeEW8AAPFLHEHDBQzxbiOCIKei3Dru5P4Q8Q0cXM+lInp4pZOJv5Ul/inRYOoT8IG63MIJgkZzz08sgCx7yJRcBNqhjfQYPQqbC4FXoPNaKjDdxwScpc9gp+luKI7xDPif0IlLRPjxQ3RUOESX+ESXyHGixtDqHQoczgkH3oo8RvFq2JrEjZYn0MwSch47uGRBYh9F4mC89Gb8Fwozd3H6/AMGuow0iEhd9kj+FmKK/q1eFSsh4PFneJGOFj8XYwuijKHcOILYYiYIcaH3mFXJOi5GNj5xE+HU1ziHxBbxStJ2GB9DsEkIeO5h0cWIPZdJAo+hbbC02hwqXGow0iHhNxlj+BnN+bfI9pFO1wo1ooaOFiMEb+Lhk4lvkrMFBN6MfHTQomvE1vEc0nYYH0OwSQh47mHRxYg9l0kCt7fyXy/Ak91xMsFcKjDSIeE3GWP4Gc35g8+820XnCKqxd09mfiKHk78eBgklohJ8K/uDfKniFoxF2p84mNDMEnIeO7hkQWIfReJgsHoJfcyuSjYEsITDk/lw6EOIx0Scpc9gp/dmP9l0Spa4HgxTdzZjcQHL8nbJya46N/hXiQfEQMdwmWf7TC/Y+KPcon/ukv8fPGs2JSEDdbnEEwSMp57eGQBYt9FiuBFtBt2ocHoaXis8/ina30ugvof6jDSISH33SP42Y35N4hXxSswXIwTY7uR+CC7Fe4F+YUwIYTyfJgnJofe6p4XCxyqHRaJO6HSJf5Il/ivusRPEU+Jx5OwwfocgklCxnMPjyxA7LtIESxFTdBQuOOPOjxSAJsd/lkAwxzOdUjIffcIfnZj/rlis3gKBonbxF+gLJTvfSJ4i9zYAm+iz8V8MS2E8nxYLKZDmVgkJkRG8BR8pajtmPivuMRPFE/6xMeDYJKQ8dzDIwsQ+y5SBNehl2GLq/ZD3cCmHDwDD6MjHM5wSMh99wh+dmP+v4sNYl0o8VXi7+I6qIyGme559oViyr6wWMxymOmCnhdBtSdFRnDwHiR+qkv8dDjJJf6v4hHxYBI2WJ9DMEnIeO7hkQWIfRcpgnPRs/CPzgf9KXgwGo5wGOGQkPvuEfzsxvw/E0vEwlDiM6gsignieqgU893hc437EroaMRMqxdJ8mOMSf48L+qx86FriK1zi3ydqxV3wZVEn5sFo8YjYkIQN1ucQTBIynnt4ZAFi30WK4Fj0BKxDgxyqouEfsB5Vocfhfvcht/flwxCHbzhYMu67R/Cz87PtFHvEHrhAzBEToCp0FB8l8XeJm9w31Ex2h89Bx+vE3S7od4fKHmBeLyZ+JnxRLBPz4ffiIbE+CRuszyGYJGQ89/DIAsS+ixTBkehhWIMq0UZYFRmPwlpUiR6BtWhwYQxx+LqDT3yS0IXZ3hAtohlOcomvcPmOiNnilo5fHL/MBX25mO2iP8cd1893WAAVokbMgXKx1JU9C2Xu+2E7hQr3FPwwUStmwYlimVgAvxYbxbokbLA+h2CSkPHcwyMLEPsuUgRDQol/EFZ2xDqoi4bBbq4iif+aQ/ve78lNykro2+jCbM+I18SrcGwo8VPc581GwXz3CvZad35+pZgDVWKlmAdV7iveAsx3qIZBYrlYAOXuH4NcDHSH4Z1ChcMwl/gvuGX9XGwU9yVhg/U5BJOEjOceHlmA2HeRXLyAdsGOjol/IFTtGlSJ7o+MTOKH5JQ9N/Fpn/gEoQuzrRJPiadgaCjxs8XtkRF8q2zm4H2QWC3mwiCx0gV9RVEEiV8hZhfAMncyPwsVEXCEwwnuH4xrxQaxNgkbrM8hmCRkPPfwyALEvovkYiFqhDfRELQhVPZFHXF/USwJYTC6H2rRYLQelrnoh/FFB5/4JKELs80Q68Q6qAglfo77APkimOuQlfjBYpVL/HIx152x7wJWuufrgxP+wYH53QVQUwCHO3xW1Ih5cIW4T6xMwgbrcwgmCRnPPTyyALHvIrn4A3oenkYVaC3Mj4Yg6Atz0NnE70btkI57JXgEPzs/2/WiViyFCvfW8i5gSuhVdoNDT8Evce96W+je4b5PZObKHOlXiDoxt+hL8orgcIfPuMT/SKwVK5KwwfocgklCxnMPjyxA7LtILs5Bj8I6VIFWu44vh3nRUIlWQDWqRGtd4u/bV+JP9IlPIrow21XuqepuJr5CLBHToUosETOgXMwRk6FMzI6M4JX5d4Wepq9w72cP53tRZGQS/2mX+MvEGrEsCRuszyGYJGQ89/DIAsS+i+TiA+618eHE17qO7xOZxA8qmvgsfM5hB9oDe+JeCR7Bz8h/vUPsFrvhO6HXp3Wq7AtDyHysTXAYnjkwj172cOKrxV0w2L3HrTL0qvtM4hdGRm7iLxVrRF0SNlifQzBJyHju4ZEFiH0XySDzJHhV0cTXREYlWg4L0CC0EhY5LEaD0ZqO+KyDT3ySEP2vt4om0eQ+1nWmOwyfGhkL8mF6CF1LfPAS/ekwyJU9OHU/p3SJv0Ss9omPB8EkIeO5h0cWIPZdJINdyKC91Imfjwah5S7xyzuW/bCcxG9DO2BH3GvDI/gZ+a//IbaKrfCR0CfV1LrD8CxUOATvVZ/mvv+lEOaLqV1N/EAx250WWCBmhFCeD9ET/ymX+AvFKlEH7cKEJWHL9RUEk4SM5x4eWYDYd5EMNqGdsD0n8Yu7kfgal/i6jokf4oI+NCfxr6JWaI17bfRtfAOAk+nEbMvEJvE0HNHJxC8pReIPEfcUxRSHqQUwR0ztTOLvgePdB+98V6wQdbBdpMWeJGzCWPBeAPpBGQD94QB3SU8tNJgkZDz38MgCJGC/3It70DZ4A1WhDbAcVbjsliTx1S7x1fkS/wmHV1AztMS9NvokKgGohNEAXEcn5p8qHhIPhT4iZp8obeLvLoopDlML4J7OJH6pmA3HizpRDRe4xDeKXWJXErZlL2A/SEMKfgSzoBLGwSw4PB+ugVkwCN4FeyBVqpsRTBIynnt4ZAESsKfuxS/Ri/BPVIXug5qOuBdqupT4eS7xAZbBPDQkB8e5xD+GXoPX414bfRKZsnch8X8Vq8Uq9/T6jAjobOIHdjz3HkapEp95mr4IhrlvnzlerBCL4Dsu8W+IHWJHErZlL+A0WAMHwzi4zgV9n/gdjIMhpboZwSQh47mHRxYgAXvqXoxET8JDRRNf5V4knzlCL4QuJ/5h9Cq8Gvfa6Es4ATbCCTAaNrrEb+xk4q8RS0R1zyd+Sj5kdbwIiic+eEneXIf5+fA+98r8T4rlYhGcJ5aLWnhd7BDbk7BRS46DAOgfwg1wCxzaycRncBqshYO7ecOCSULGcw+PLIjYb4eBMNCH4WasDZ0Dc7FWdG4+zMFa0LdhKrYT/SdMyYft6LsO/wVTsFb0XzAVa0EXwuSO+CXcgLWg98DhGOiI+DfPOxvlsAmOhqshBWlQDt56nBbAdvdeuf8UC91J+IWu40UQ8d+AzGvhprr3xedF8XxHQZD4Kveq+yr3UrpcDHOJ/7hL/EixXCyFZ93bCoqssbcrbgbgsBDGwS2dL3sGY2EtHNbNGxb7buThUQQi9tuxHfbHQINgbmcSfzfWiv4DZmEt6P/ATKwFXQDTsRb0XYfcoBdJ/G74IAYaHv/mKYSjYAUMh8vhA3AF/Ab+Ba6Fy2Eg/ASudLgAjoEroAFaYX+wZNyLn8FhsAUO7kbiXxRNohFOFgvdm92ioEjZa9y/CgHKRbWYDgPFvAIIYj2tG7hHTIMq9+20AebmwzBRK+bBx9xz8WeJOrEEnnFro8gae7vitwAMgXGAi3V3Ej8O1sLh3bxhse9GHh7JTvyb8F4MNDRa4udiLeg8mIc1o2/DXIc5WDM6PyfxmYP3XHwfpoQSvx1thU9gQsfEv3my8BfYH34Pv4AGGAM/7wyuhTb4KxwGt8Hx0Brr3bmucNkVa+Jr3QfI17jXuWUSP78w7imKGdHgE79v/BGAoT7xHh77hIj9djwIw7F2dEzHxO8TS7EWdAYsw5rRqbAca0anhC6pw1rQ6VBTACPc9YyC27EWtBE+jxk6Nv7NE+BmaIEbOhn0KHg31MDRsLMX784X4bfwU/jq2zDxCwpjTgEEJ/NnRoNP/L6RSfyf4BbXep94D488+P+PX7Ncn7XJuQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224 at 0x7F0FC0299320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_IBjmoSyWdT",
        "outputId": "65a25ee5-8f39-4482-a1b8-c66cc60d628f"
      },
      "source": [
        "prediction"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'boxes': tensor([[  0.0000,  88.0800,  27.6211, 106.8034],\n",
              "          [  0.0000,   8.2850, 151.5132, 162.8147],\n",
              "          [  0.0000,  75.7268, 224.0000, 156.6401],\n",
              "          [ 17.9723,  60.7403, 185.7222, 138.0727],\n",
              "          [  0.0000, 170.9290, 124.7929, 200.4859],\n",
              "          [ 22.5377,  27.6003, 220.2709,  63.3934],\n",
              "          [ 16.7467,  38.7473, 128.0023,  99.6804],\n",
              "          [185.1766, 185.2156, 211.4926, 210.5097],\n",
              "          [166.7910, 150.3871, 208.7434, 207.0043],\n",
              "          [ 21.9000,  37.0759, 218.2239, 195.6512],\n",
              "          [ 42.7215,  64.1217, 205.9946,  96.2052],\n",
              "          [ 10.4787,  57.7156, 175.5271,  87.4769],\n",
              "          [  0.0000, 157.1068, 146.3099, 186.9339],\n",
              "          [  0.0000,  10.3850, 148.4418, 172.2945],\n",
              "          [134.8318,  69.4643, 204.8895, 136.3717],\n",
              "          [ 20.7710,  45.5430, 197.1770,  79.7840],\n",
              "          [  0.0000,  41.0303, 224.0000,  96.2940],\n",
              "          [  9.0015,  66.4285, 132.7011,  95.5708],\n",
              "          [ 98.3184, 143.5949, 206.1450, 209.1509],\n",
              "          [  0.0000, 174.9047, 224.0000, 201.7202],\n",
              "          [ 28.1691,  72.4766, 176.0929, 105.6953],\n",
              "          [  0.0000, 147.9722, 135.8662, 176.5365],\n",
              "          [178.4885, 158.1706, 218.0835, 188.5982],\n",
              "          [ 11.4665,  36.8627, 121.6272,  69.3348],\n",
              "          [ 82.8445,  30.4623, 219.8576,  80.1843],\n",
              "          [157.6230,  80.3659, 208.3847,  99.1860],\n",
              "          [ 76.9650, 161.4381, 224.0000, 195.0516],\n",
              "          [138.7350, 152.3854, 216.0198, 168.7136],\n",
              "          [177.8238,  62.5955, 221.0230,  91.9051],\n",
              "          [178.5282,  49.6646, 218.2777,  80.9478],\n",
              "          [ 61.5111,  41.3490, 168.7013,  92.5003],\n",
              "          [  0.0000, 145.7932, 223.4282, 201.4359],\n",
              "          [ 82.3082, 143.7294, 223.8451, 176.4575],\n",
              "          [ 20.6886, 162.5477, 181.8868, 196.2190],\n",
              "          [ 68.3431,  22.3075, 217.0983,  53.4296],\n",
              "          [ 47.6684,  21.1726, 149.3069,  82.0487],\n",
              "          [116.2082, 152.8118, 215.7107, 215.4646],\n",
              "          [ 10.4896,  43.6776, 110.7943,  78.9967],\n",
              "          [  6.8038, 146.4931,  21.8417, 174.7808],\n",
              "          [ 63.2399, 146.9206, 166.8575, 208.7138],\n",
              "          [  9.6007,  63.9922, 119.6778,  97.4667],\n",
              "          [ 16.9205,  54.4353, 163.7127,  89.4599],\n",
              "          [  0.0000,  62.8774, 224.0000, 151.3847],\n",
              "          [  8.9614,  25.7675, 106.5972,  63.1163],\n",
              "          [ 18.6345, 140.4334, 130.4063, 193.3761],\n",
              "          [ 86.3055, 177.0867, 224.0000, 216.7740]], device='cuda:0'),\n",
              "  'labels': tensor([3, 3, 1, 3, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1,\n",
              "          1, 3, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 3, 3, 3, 1, 3, 3, 2, 3, 1, 3],\n",
              "         device='cuda:0'),\n",
              "  'scores': tensor([0.3395, 0.2070, 0.1598, 0.1403, 0.1376, 0.1155, 0.1136, 0.1130, 0.1121,\n",
              "          0.1121, 0.1041, 0.0998, 0.0970, 0.0969, 0.0920, 0.0905, 0.0894, 0.0871,\n",
              "          0.0864, 0.0858, 0.0858, 0.0811, 0.0797, 0.0790, 0.0784, 0.0782, 0.0772,\n",
              "          0.0766, 0.0727, 0.0702, 0.0701, 0.0678, 0.0655, 0.0614, 0.0595, 0.0589,\n",
              "          0.0568, 0.0564, 0.0559, 0.0547, 0.0537, 0.0535, 0.0518, 0.0510, 0.0506,\n",
              "          0.0504], device='cuda:0')}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M58J3O9OtT1G"
      },
      "source": [
        "And let's now visualize the top predicted segmentation mask. The masks are predicted as `[N, 1, H, W]`, where `N` is the number of predictions, and are probability maps between 0-1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v5S3bm07SO1"
      },
      "source": [
        "Image.fromarray(prediction[0]['boxes'][0, 0].mul(255).byte().cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EZCVtCPunrT"
      },
      "source": [
        "Looks pretty good!\n",
        "\n",
        "## Wrapping up\n",
        "\n",
        "In this tutorial, you have learned how to create your own training pipeline for instance segmentation models, on a custom dataset.\n",
        "For that, you wrote a `torch.utils.data.Dataset` class that returns the images and the ground truth boxes and segmentation masks. You also leveraged a Mask R-CNN model pre-trained on COCO train2017 in order to perform transfer learning on this new dataset.\n",
        "\n",
        "For a more complete example, which includes multi-machine / multi-gpu training, check `references/detection/train.py`, which is present in the [torchvision GitHub repo](https://github.com/pytorch/vision/tree/v0.3.0/references/detection). \n",
        "\n"
      ]
    }
  ]
}